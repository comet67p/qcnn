{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbca1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define constants\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss, MSELoss\n",
    "\n",
    "# Run specific\n",
    "DATASET = \"cadena\" # \"mnist\", \"fashion-mnist\", \"cifar\"\n",
    "EPOCHS = 20\n",
    "QUBITS = 2\n",
    "NEURONS_FROM=0\n",
    "NEURONS_PREDICTED=1\n",
    "SIMPLE_NEURONS_PER_FEATURE_PER_ROW = 8\n",
    "LOSS_FUNC = MSELoss(reduction=\"sum\")\n",
    "PERTURB = False\n",
    "RESUME_TRAINING = True\n",
    "CHOOSE_NEW_RECEPTIVE_FIELDS = False\n",
    "CHOOSE_NEW_TRAINING_DATA = False\n",
    "TRAINING_RUN = \"v83\"\n",
    "# Constants\n",
    "QC_REPETITIONS = 1\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "FILENAME_PREFIX = f'v3-{DATASET}-r{TRAINING_RUN}-q{QUBITS}-{QC_REPETITIONS}'\n",
    "\n",
    "if DATASET == \"cifar\":\n",
    "  IMAGE_WIDTH = 32\n",
    "  TARGETS = np.array([\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"])  \n",
    "elif DATASET == \"fashion-mnist\":\n",
    "  IMAGE_WIDTH = 28\n",
    "  TARGETS = np.array([\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]) \n",
    "elif DATASET == \"cadena\":\n",
    "   IMAGE_WIDTH = 40\n",
    "   TARGETS = []\n",
    "else: \n",
    "  IMAGE_WIDTH = 28\n",
    "  TARGETS = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "## Settings\n",
    "DEBUG = False\n",
    "SAMPLE_RUN = False\n",
    "SAMPLE_ITERATIONS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "banned-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Necessary imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from itertools import cycle\n",
    "import random\n",
    "import os.path as path\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.quantum_info.operators.base_operator import BaseOperator\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional torch-related imports\n",
    "import torch\n",
    "from torch import Tensor, cat, no_grad, manual_seed\n",
    "from torch.nn import Linear, MSELoss, Softmax\n",
    "from torch.optim import LBFGS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Conv2d,\n",
    "    Linear,\n",
    "    Bilinear,\n",
    "    Dropout2d,\n",
    "    MaxPool2d,\n",
    "    Flatten,\n",
    "    Sequential,\n",
    "    ReLU,\n",
    "    AvgPool2d,\n",
    "    ELU\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Project specific imports\n",
    "from canonicallayers.brightnessextractor import extractBrightness, extractBrightnessAtV2\n",
    "from canonicallayers.gaborfilterextractor import extract_power_featsV2\n",
    "from canonicallayers.perturbations import skew, rotate, noise, elastic_transform\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "from cadena_ploscb.data import Dataset, MonkeySubDataset\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "aersim = AerSimulator()\n",
    "\n",
    "algorithm_globals.random_seed = 42\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worthy-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracting mean: 112.51203\n",
      "(18560, 166)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare Training data\n",
    "# -------------\n",
    "\n",
    "data_dict = Dataset.get_clean_data()\n",
    "dataset = MonkeySubDataset(data_dict, seed=1000, train_frac=0.8, subsample=2, crop=30)\n",
    "\n",
    "# b_out is calculated as inverse ELU of responses mean\n",
    "# Used as input for non-linear mapping the model output to neuron firing rates\n",
    "_, responses, _ = dataset.train()\n",
    "print(responses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "medieval-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT = True\n",
    "# BATCH_SIZE = 256\n",
    "# if(PLOT):\n",
    "#   rows = 10\n",
    "#   fig, axes = plt.subplots(nrows=rows, ncols=rows, sharex=True, figsize=(20, 20))\n",
    "#   img_batch, res_batch, real_batch = dataset.minibatch(BATCH_SIZE)\n",
    "#   print(\"img_batch\", img_batch.shape)\n",
    "#   images = torch.tensor(img_batch).permute(0, 3, 1, 2)\n",
    "#   print(\"images\", images.shape)\n",
    "#   print(\"squeezed\", images[0].squeeze().shape)\n",
    "#   for p in range(rows * rows):\n",
    "#     axes[p//rows,p%rows].imshow(images[p].squeeze(), cmap=\"gray\")\n",
    "#     axes[p//rows,p%rows].set_xticks([])\n",
    "#     axes[p//rows,p%rows].set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "urban-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and QNN Model with configurable number of qubits\n",
    "# Second-order Pauli-Z evolution circuit ZZFeatureMap with repetitions as feature map\n",
    "# Real amplitudes as Ansatz\n",
    "# Quantum observabls as predictions\n",
    "# Track gradients for training using Pytorch Autograd\n",
    "\n",
    "# Define and create QNN\n",
    "def create_qnn():\n",
    "    feature_map = ZZFeatureMap(QUBITS, reps=QC_REPETITIONS)\n",
    "    ansatz = RealAmplitudes(QUBITS, reps=1)\n",
    "    qc = QuantumCircuit(QUBITS)\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        input_gradients=True\n",
    "    )\n",
    "    return qnn\n",
    "\n",
    "\n",
    "qnn1 = create_qnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exclusive-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hybrid torch NN Module\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, qnn1):\n",
    "      super().__init__()\n",
    "      self.conv1 = Conv2d(1, 32, kernel_size=13,padding='valid').to(device)\n",
    "      self.pool = AvgPool2d(kernel_size=5, stride=1, padding=2).to(device)\n",
    "      self.fc1 = Linear(32*28*28, QUBITS).to(device)  # 32 * QUBIT-dimensional output\n",
    "      # self.fc2 = Linear(32*QUBITS, QUBITS).to(device)  # QUBIT-dimensional output\n",
    "      self.elu = ELU()\n",
    "      self.qnn1 = TorchConnector(qnn1).to(device)  # Apply torch connector, weights chosen\n",
    "      self.relu = ReLU().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = x.permute(0, 3, 1, 2)\n",
    "      if(DEBUG):\n",
    "        print(\"x shape\", x.shape)\n",
    "      x = self.elu(self.conv1(x))\n",
    "      # print(\"conv1\", x[0, 0, 0, 0])\n",
    "      x = self.elu(self.pool(x))\n",
    "      # print(\"pool\", x[0, 0, 0, 0])\n",
    "      x = self.elu(self.fc1(torch.flatten(x, start_dim=1).to(device)))\n",
    "      # print(\"fc1\", x[0, 0])\n",
    "      # x = self.relu(self.fc2(x))\n",
    "      # print(\"fc2\", x[0, 0])\n",
    "      x = torch.abs(self.qnn1(x)).to(device) * 3\n",
    "      # print(\"qnn\", x[0, 0])\n",
    "      if(DEBUG):\n",
    "        print(\"After qnn, output\", x.shape, x[0:3, :])\n",
    "      return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a912544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 - Initialize model\n",
    "\n",
    "model = Net(qnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eaac40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 - Load previous results\n",
    "\n",
    "loss_list = []  # Store loss history\n",
    "training_stats = [0, 0.0, 0] #Epochs, Accuracy, Training Time\n",
    "resume_stats = [-1, -1]\n",
    "if(RESUME_TRAINING and (path.isfile(f'checkpoints/{FILENAME_PREFIX}-modelweights.pt'))):\n",
    "  model.load_state_dict(torch.load(f'checkpoints/{FILENAME_PREFIX}-modelweights.pt'))\n",
    "  loss_list = torch.load(f\"checkpoints/{FILENAME_PREFIX}-losslist.pt\")\n",
    "if(path.isfile(f'checkpoints/{FILENAME_PREFIX}-trainingstats.pt')):\n",
    "  training_stats = torch.load(f\"checkpoints/{FILENAME_PREFIX}-trainingstats.pt\")\n",
    "if(path.isfile(f'checkpoints/{FILENAME_PREFIX}-trainresumestats.pt')):\n",
    "  resume_stats = torch.load(f\"checkpoints/{FILENAME_PREFIX}-trainresumestats.pt\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "860baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Epochs 0 Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 13 - Plot Prior Losses\n",
    "print(\"Completed Epochs\", training_stats[0], \"Accuracy\", training_stats[1])\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"Hybrid NN Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "precious-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cadena 2 Qubits,  0 prior epochs,  Accuracy 0.0% Epochs 20 Batch Size: 256\n",
      "0 0 already processed\n",
      "0 1 already processed\n",
      "0 2 already processed\n",
      "0 3 already processed\n",
      "0 4 already processed\n",
      "0 5 output [0.60523415] target [3.] loss 511.918701171875\n",
      "0 6 output [0.73951995] target [0.] loss 466.74029541015625\n",
      "0 7 output [2.6964831] target [2.] loss 433.8669738769531\n",
      "0 8 output [2.0842524] target [2.] loss 445.520263671875\n",
      "0 9 output [1.4553885] target [1.] loss 421.4096374511719\n",
      "0 10 output [2.1163757] target [1.] loss 407.0358581542969\n",
      "0 11 output [0.0973852] target [0.] loss 396.61358642578125\n",
      "0 12 output [0.04529984] target [1.] loss 391.80877685546875\n",
      "0 13 output [0.10771856] target [2.] loss 399.47711181640625\n",
      "0 14 output [2.5598915] target [1.] loss 408.01116943359375\n",
      "0 15 output [2.1877327] target [0.] loss 419.4415283203125\n",
      "0 16 output [0.1561376] target [0.] loss 393.45147705078125\n",
      "0 17 output [1.1333872] target [3.] loss 413.58599853515625\n",
      "0 18 output [2.0893846] target [5.] loss 470.2209777832031\n",
      "0 19 output [2.3491657] target [2.] loss 381.417724609375\n",
      "0 20 output [0.65956604] target [0.] loss 417.1412658691406\n",
      "0 21 output [0.30562657] target [1.] loss 434.41851806640625\n",
      "0 22 output [1.5310018] target [2.] loss 466.1163330078125\n",
      "0 23 output [1.25249] target [4.] loss 376.77008056640625\n",
      "0 24 output [0.14710923] target [1.] loss 390.5933532714844\n",
      "0 25 output [1.5223793] target [1.] loss 402.0862731933594\n",
      "0 26 output [1.5194446] target [3.] loss 434.31036376953125\n",
      "0 27 output [0.38904232] target [0.] loss 428.0086364746094\n",
      "0 28 output [0.5158897] target [2.] loss 430.91815185546875\n",
      "0 29 output [1.4978588] target [1.] loss 400.71441650390625\n",
      "0 30 output [0.15057987] target [1.] loss 377.4716491699219\n",
      "0 31 output [2.435303] target [1.] loss 466.86981201171875\n",
      "0 32 output [1.107573] target [1.] loss 396.7584533691406\n",
      "0 33 output [1.9093623] target [1.] loss 368.41351318359375\n",
      "0 34 output [1.4970996] target [1.] loss 394.7925720214844\n",
      "0 35 output [1.4941902] target [0.] loss 403.7467041015625\n",
      "0 36 output [2.610357] target [1.] loss 402.9110107421875\n",
      "0 37 output [1.0845752] target [2.] loss 342.4621887207031\n",
      "0 38 output [1.7786615] target [1.] loss 390.4892272949219\n",
      "0 39 output [0.65858877] target [2.] loss 396.48980712890625\n",
      "0 40 output [1.214485] target [1.] loss 374.81060791015625\n",
      "0 41 output [1.477548] target [0.] loss 431.18798828125\n",
      "0 42 output [2.3549116] target [4.] loss 471.4951171875\n",
      "0 43 output [1.3168063] target [1.] loss 386.16656494140625\n",
      "0 44 output [1.4692992] target [2.] loss 427.00311279296875\n",
      "0 45 output [1.7107219] target [2.] loss 359.69024658203125\n",
      "0 46 output [1.991374] target [0.] loss 443.020263671875\n",
      "0 47 output [1.460785] target [1.] loss 356.2672119140625\n",
      "0 48 output [0.16822928] target [2.] loss 432.57086181640625\n",
      "0 49 output [1.4089857] target [1.] loss 425.10894775390625\n",
      "0 50 output [2.0672827] target [2.] loss 343.5970764160156\n",
      "0 51 output [1.4494722] target [3.] loss 388.612548828125\n",
      "0 52 output [1.8476558] target [0.] loss 430.3133850097656\n",
      "0 53 output [2.7127867] target [2.] loss 409.2147216796875\n",
      "0 54 output [1.4404874] target [0.] loss 439.7189025878906\n",
      "0 55 output [2.0443943] target [0.] loss 386.47607421875\n",
      "0 56 output [0.6783341] target [1.] loss 394.98333740234375\n",
      "0 57 output [1.4397532] target [0.] loss 410.29541015625\n",
      "0 58 output [1.4297843] target [2.] loss 396.77880859375\n",
      "0 59 output [1.4272414] target [2.] loss 453.9058837890625\n",
      "0 60 output [1.4247828] target [0.] loss 462.12445068359375\n",
      "0 61 output [1.778871] target [0.] loss 410.4406433105469\n",
      "0 62 output [1.6271551] target [0.] loss 417.968505859375\n",
      "0 63 output [1.417202] target [2.] loss 498.4647521972656\n",
      "0 64 output [1.0969672] target [1.] loss 293.864990234375\n",
      "0 65 output [1.2040585] target [0.] loss 476.856201171875\n",
      "0 66 output [1.4108437] target [1.] loss 374.8003845214844\n",
      "0 67 output [0.05068982] target [2.] loss 455.2851257324219\n",
      "0 68 output [1.7410938] target [3.] loss 441.45452880859375\n",
      "0 69 output [2.252366] target [2.] loss 435.7916564941406\n",
      "0 70 output [1.5431757] target [1.] loss 452.3468017578125\n",
      "0 71 output [0.2070109] target [1.] loss 441.5284423828125\n",
      "0 72 output [0.24298252] target [0.] loss 408.2272033691406\n",
      "Training [5%]\tLoss: 408.2272 Elapsed 305.832688 total elapsed 305.832688\n",
      "1 0 output [0.34719953] target [1.] loss 411.91009521484375\n",
      "1 1 output [1.3951653] target [0.] loss 432.36676025390625\n",
      "1 2 output [2.8455343] target [1.] loss 410.579345703125\n",
      "1 3 output [0.5334818] target [1.] loss 434.7723388671875\n",
      "1 4 output [1.9778936] target [0.] loss 417.71435546875\n",
      "1 5 output [1.1132565] target [1.] loss 357.2747802734375\n",
      "1 6 output [1.9020317] target [2.] loss 407.6772155761719\n",
      "1 7 output [1.3754911] target [0.] loss 401.5914611816406\n",
      "1 8 output [1.3835425] target [1.] loss 447.6876525878906\n",
      "1 9 output [0.861748] target [0.] loss 385.5738525390625\n",
      "1 10 output [1.3807834] target [1.] loss 342.162109375\n",
      "1 11 output [1.2164459] target [0.] loss 415.9022216796875\n",
      "1 12 output [1.3775092] target [2.] loss 324.1937255859375\n",
      "1 13 output [0.75670063] target [4.] loss 447.7308654785156\n",
      "1 14 output [2.2039752] target [2.] loss 394.83538818359375\n",
      "1 15 output [1.7222164] target [0.] loss 416.84136962890625\n",
      "1 16 output [2.0130775] target [0.] loss 460.47088623046875\n",
      "1 17 output [0.23117763] target [1.] loss 412.7955322265625\n",
      "1 18 output [1.5355508] target [1.] loss 384.8013916015625\n",
      "1 19 output [1.3635104] target [1.] loss 418.1641845703125\n",
      "1 20 output [2.088203] target [2.] loss 424.4244384765625\n",
      "1 21 output [2.1960382] target [2.] loss 382.3248596191406\n",
      "1 22 output [1.3590053] target [2.] loss 445.3420715332031\n",
      "1 23 output [0.05814832] target [1.] loss 448.3539123535156\n",
      "1 24 output [1.2476399] target [0.] loss 431.9758605957031\n",
      "1 25 output [2.2642105] target [0.] loss 356.6092224121094\n",
      "1 26 output [0.28339598] target [2.] loss 405.8459777832031\n",
      "1 27 output [1.3409212] target [1.] loss 444.9927978515625\n",
      "1 28 output [2.1180637] target [1.] loss 403.73175048828125\n",
      "1 29 output [2.2267394] target [1.] loss 338.000732421875\n",
      "1 30 output [1.3335747] target [0.] loss 369.31341552734375\n",
      "1 31 output [0.4455334] target [2.] loss 391.199462890625\n",
      "1 32 output [1.3271859] target [2.] loss 416.338134765625\n",
      "1 33 output [2.311847] target [0.] loss 372.06298828125\n",
      "1 34 output [1.3275945] target [1.] loss 332.65869140625\n",
      "1 35 output [1.1017334] target [0.] loss 395.0556335449219\n",
      "1 36 output [0.04565671] target [1.] loss 405.47882080078125\n",
      "1 37 output [1.3201699] target [0.] loss 341.53265380859375\n",
      "1 38 output [1.3176446] target [1.] loss 439.397705078125\n",
      "1 39 output [0.5488874] target [2.] loss 437.1617126464844\n",
      "1 40 output [1.1236167] target [0.] loss 336.3587646484375\n",
      "1 41 output [1.310592] target [5.] loss 392.95220947265625\n",
      "1 42 output [1.30802] target [0.] loss 374.40234375\n",
      "1 43 output [0.35997427] target [3.] loss 371.9858703613281\n",
      "1 44 output [1.3028593] target [1.] loss 390.46368408203125\n",
      "1 45 output [1.2995719] target [1.] loss 382.3408203125\n",
      "1 46 output [1.2978876] target [3.] loss 363.6256408691406\n",
      "1 47 output [1.2952253] target [2.] loss 402.36505126953125\n",
      "1 48 output [1.13989] target [0.] loss 424.10418701171875\n",
      "1 49 output [2.2648742] target [1.] loss 390.09869384765625\n",
      "1 50 output [1.542777] target [0.] loss 424.1459045410156\n",
      "1 51 output [1.2864459] target [1.] loss 393.01434326171875\n",
      "1 52 output [1.2842448] target [1.] loss 352.0372314453125\n",
      "1 53 output [1.2818522] target [3.] loss 382.3443603515625\n",
      "1 54 output [1.4508811] target [1.] loss 320.5657958984375\n",
      "1 55 output [1.2765744] target [3.] loss 454.34661865234375\n",
      "1 56 output [1.2680266] target [0.] loss 352.662109375\n",
      "1 57 output [2.8505416] target [2.] loss 461.3771057128906\n",
      "1 58 output [1.2669145] target [1.] loss 402.2066650390625\n",
      "1 59 output [1.2669127] target [0.] loss 401.9444885253906\n",
      "1 60 output [1.5110731] target [0.] loss 419.9250183105469\n",
      "1 61 output [1.9960095] target [1.] loss 353.9390563964844\n",
      "1 62 output [1.2599972] target [1.] loss 398.68902587890625\n",
      "1 63 output [0.8440416] target [2.] loss 394.8507080078125\n",
      "1 64 output [0.26003397] target [2.] loss 372.4767150878906\n",
      "1 65 output [1.0579733] target [1.] loss 401.5262451171875\n",
      "1 66 output [1.1556705] target [0.] loss 505.1529846191406\n",
      "1 67 output [1.2265035] target [1.] loss 372.6451416015625\n",
      "1 68 output [0.24726462] target [2.] loss 355.88861083984375\n",
      "1 69 output [0.582604] target [2.] loss 405.77557373046875\n",
      "1 70 output [1.7174358] target [2.] loss 390.08477783203125\n",
      "1 71 output [1.2414412] target [0.] loss 371.96649169921875\n",
      "1 72 output [1.242823] target [0.] loss 390.7034912109375\n",
      "Training [10%]\tLoss: 390.7035 Elapsed 626.173511 total elapsed 932.0061989999999\n",
      "2 0 output [2.1921701] target [0.] loss 409.8321533203125\n",
      "2 1 output [1.2397642] target [0.] loss 352.1851806640625\n",
      "2 2 output [1.2382423] target [2.] loss 397.2203369140625\n",
      "2 3 output [0.4064566] target [0.] loss 382.03936767578125\n",
      "2 4 output [1.235557] target [1.] loss 318.2820739746094\n",
      "2 5 output [1.2342649] target [0.] loss 405.4263610839844\n",
      "2 6 output [0.28008938] target [0.] loss 350.348876953125\n",
      "2 7 output [0.05115779] target [0.] loss 394.50872802734375\n",
      "2 8 output [1.0248274] target [0.] loss 370.2435607910156\n",
      "2 9 output [1.2284155] target [2.] loss 365.6213684082031\n",
      "2 10 output [1.226479] target [0.] loss 404.01776123046875\n",
      "2 11 output [1.2248313] target [0.] loss 410.81207275390625\n",
      "2 12 output [0.11403367] target [2.] loss 433.6715087890625\n",
      "2 13 output [1.2199812] target [0.] loss 359.94573974609375\n",
      "2 14 output [1.2201724] target [0.] loss 385.45281982421875\n",
      "2 15 output [1.218688] target [1.] loss 401.9304504394531\n",
      "2 16 output [0.53842616] target [3.] loss 402.76177978515625\n",
      "2 17 output [1.214924] target [1.] loss 336.2063903808594\n",
      "2 18 output [0.9546778] target [2.] loss 402.7335510253906\n",
      "2 19 output [1.2115561] target [0.] loss 427.0176696777344\n",
      "2 20 output [2.4694052] target [0.] loss 423.74652099609375\n",
      "2 21 output [1.2078967] target [2.] loss 455.7449645996094\n",
      "2 22 output [0.986796] target [0.] loss 380.7421875\n",
      "2 23 output [1.9523041] target [2.] loss 381.7791442871094\n",
      "2 24 output [0.06538553] target [0.] loss 391.3238525390625\n",
      "2 25 output [1.2038639] target [0.] loss 438.1455078125\n",
      "2 26 output [2.0195098] target [0.] loss 455.1424255371094\n",
      "2 27 output [2.1218147] target [0.] loss 470.363037109375\n",
      "2 28 output [1.2024589] target [1.] loss 389.10247802734375\n",
      "2 29 output [1.2552027] target [2.] loss 402.6983337402344\n",
      "2 30 output [0.949961] target [0.] loss 391.5889587402344\n",
      "2 31 output [1.1516554] target [1.] loss 363.09393310546875\n",
      "2 32 output [1.8350041] target [0.] loss 379.39410400390625\n",
      "2 33 output [1.1964519] target [2.] loss 390.22174072265625\n",
      "2 34 output [0.48515636] target [2.] loss 419.15264892578125\n",
      "2 35 output [1.5701519] target [3.] loss 364.75244140625\n",
      "2 36 output [0.25450796] target [0.] loss 360.6927185058594\n",
      "2 37 output [1.2009861] target [0.] loss 412.0398254394531\n",
      "2 38 output [1.7392697] target [0.] loss 369.058349609375\n",
      "2 39 output [1.902451] target [1.] loss 407.391845703125\n",
      "2 40 output [1.1993749] target [3.] loss 392.3898620605469\n",
      "2 41 output [1.9349589] target [0.] loss 328.2392578125\n",
      "2 42 output [2.5717022] target [1.] loss 453.47149658203125\n",
      "2 43 output [1.1430506] target [0.] loss 370.8081359863281\n",
      "2 44 output [1.0550823] target [1.] loss 392.5170593261719\n",
      "2 45 output [1.1862515] target [0.] loss 387.53033447265625\n",
      "2 46 output [1.1412842] target [2.] loss 368.4959716796875\n",
      "2 47 output [2.4160862] target [0.] loss 405.8817443847656\n",
      "2 48 output [0.9392302] target [1.] loss 402.2940368652344\n",
      "2 49 output [1.1543797] target [1.] loss 383.61090087890625\n",
      "2 50 output [0.04981073] target [0.] loss 344.57879638671875\n",
      "2 51 output [1.085179] target [1.] loss 368.22625732421875\n",
      "2 52 output [1.1146394] target [1.] loss 374.4359436035156\n",
      "2 53 output [1.1890223] target [2.] loss 530.9013671875\n",
      "2 54 output [1.7869725] target [2.] loss 411.86328125\n",
      "2 55 output [1.1966649] target [3.] loss 358.35205078125\n",
      "2 56 output [1.1729655] target [0.] loss 371.2711181640625\n",
      "2 57 output [0.97852635] target [0.] loss 367.4183654785156\n",
      "2 58 output [1.9564621] target [2.] loss 369.11859130859375\n",
      "2 59 output [0.7962228] target [0.] loss 402.8813781738281\n",
      "2 60 output [1.6274781] target [3.] loss 323.7461853027344\n",
      "2 61 output [1.497266] target [3.] loss 400.8381652832031\n",
      "2 62 output [0.9470477] target [0.] loss 378.1339416503906\n",
      "2 63 output [1.168024] target [0.] loss 421.1689453125\n",
      "2 64 output [1.6753373] target [1.] loss 448.16357421875\n",
      "2 65 output [2.4019408] target [0.] loss 460.082275390625\n",
      "2 66 output [0.84652287] target [1.] loss 411.8990783691406\n",
      "2 67 output [1.0736809] target [2.] loss 365.78875732421875\n",
      "2 68 output [1.1901608] target [2.] loss 401.4408264160156\n",
      "2 69 output [0.87545425] target [1.] loss 374.49053955078125\n",
      "2 70 output [0.99580395] target [1.] loss 382.16046142578125\n",
      "2 71 output [0.89420074] target [0.] loss 496.2338562011719\n",
      "2 72 output [1.1897664] target [1.] loss 356.1022644042969\n",
      "Training [15%]\tLoss: 356.1023 Elapsed 925.732428 total elapsed 1857.738627\n",
      "3 0 output [1.1560525] target [0.] loss 404.07025146484375\n",
      "3 1 output [0.94751847] target [0.] loss 440.9161682128906\n",
      "3 2 output [1.1130842] target [1.] loss 471.51434326171875\n",
      "3 3 output [1.1188129] target [2.] loss 409.3382873535156\n",
      "3 4 output [1.5238304] target [0.] loss 395.18798828125\n",
      "3 5 output [0.02115271] target [2.] loss 386.8247985839844\n",
      "3 6 output [1.8657188] target [0.] loss 374.2332763671875\n",
      "3 7 output [1.3243612] target [0.] loss 351.0093994140625\n",
      "3 8 output [1.1977645] target [0.] loss 398.44915771484375\n",
      "3 9 output [1.9948858] target [1.] loss 322.79248046875\n",
      "3 10 output [1.1886548] target [2.] loss 404.0664367675781\n",
      "3 11 output [1.1885146] target [4.] loss 421.53802490234375\n",
      "3 12 output [0.46827716] target [2.] loss 367.3690185546875\n",
      "3 13 output [0.52336204] target [0.] loss 370.26177978515625\n",
      "3 14 output [1.3269656] target [2.] loss 413.1988525390625\n",
      "3 15 output [1.1887869] target [1.] loss 401.0764465332031\n",
      "3 16 output [0.03225201] target [1.] loss 379.4588317871094\n",
      "3 17 output [0.91066] target [0.] loss 357.35626220703125\n",
      "3 18 output [1.1855822] target [0.] loss 346.39666748046875\n",
      "3 19 output [1.1702125] target [1.] loss 436.124755859375\n",
      "3 20 output [1.189221] target [0.] loss 339.8521423339844\n",
      "3 21 output [2.0071812] target [1.] loss 363.55804443359375\n",
      "3 22 output [1.748934] target [1.] loss 364.624755859375\n",
      "3 23 output [1.4151311] target [2.] loss 394.6350402832031\n",
      "3 24 output [0.87500083] target [0.] loss 380.7145080566406\n",
      "3 25 output [1.7614832] target [2.] loss 376.3319091796875\n",
      "3 26 output [1.1897768] target [0.] loss 398.67388916015625\n",
      "3 27 output [1.5344853] target [4.] loss 354.46630859375\n",
      "3 28 output [1.1788433] target [1.] loss 354.75115966796875\n",
      "3 29 output [0.49616796] target [1.] loss 372.72503662109375\n",
      "3 30 output [1.4285363] target [1.] loss 352.368408203125\n",
      "3 31 output [0.1096116] target [0.] loss 366.61505126953125\n",
      "3 32 output [1.4648062] target [2.] loss 325.01434326171875\n",
      "3 33 output [1.0201178] target [1.] loss 405.66119384765625\n",
      "3 34 output [1.189179] target [3.] loss 386.65191650390625\n",
      "3 35 output [1.3728313] target [2.] loss 340.9984130859375\n",
      "3 36 output [0.8056167] target [0.] loss 395.6726989746094\n",
      "3 37 output [1.1731908] target [0.] loss 441.63134765625\n",
      "3 38 output [1.1725411] target [0.] loss 342.97149658203125\n",
      "3 39 output [1.1895708] target [1.] loss 384.8441162109375\n",
      "3 40 output [1.1799395] target [0.] loss 396.4528503417969\n",
      "3 41 output [1.1855268] target [2.] loss 406.37554931640625\n",
      "3 42 output [1.1904837] target [2.] loss 320.82281494140625\n",
      "3 43 output [1.1908718] target [2.] loss 340.65985107421875\n",
      "3 44 output [2.642256] target [1.] loss 355.81341552734375\n",
      "3 45 output [1.8848923] target [1.] loss 351.88165283203125\n",
      "3 46 output [0.9167309] target [2.] loss 344.8509216308594\n",
      "3 47 output [1.1744651] target [2.] loss 383.07928466796875\n",
      "3 48 output [0.13289842] target [2.] loss 331.6956787109375\n",
      "3 49 output [0.76702094] target [2.] loss 374.82989501953125\n",
      "3 50 output [1.7407112] target [0.] loss 408.0640869140625\n",
      "3 51 output [2.3622165] target [0.] loss 405.85394287109375\n",
      "3 52 output [1.1904128] target [1.] loss 388.7372741699219\n",
      "3 53 output [1.1904209] target [1.] loss 347.37548828125\n",
      "3 54 output [1.0581834] target [1.] loss 369.90234375\n",
      "3 55 output [0.45390218] target [0.] loss 389.54852294921875\n",
      "3 56 output [1.5395858] target [1.] loss 389.19219970703125\n",
      "3 57 output [1.2317404] target [3.] loss 390.519287109375\n",
      "3 58 output [0.16270256] target [2.] loss 482.6007080078125\n",
      "3 59 output [1.189174] target [1.] loss 334.6409912109375\n",
      "3 60 output [1.681246] target [0.] loss 366.69512939453125\n",
      "3 61 output [1.4997772] target [3.] loss 345.3309326171875\n",
      "3 62 output [1.8337109] target [2.] loss 419.8442077636719\n",
      "3 63 output [1.49014] target [0.] loss 441.3509521484375\n",
      "3 64 output [1.1890209] target [1.] loss 351.17608642578125\n",
      "3 65 output [1.1499653] target [3.] loss 360.3223876953125\n",
      "3 66 output [1.9858706] target [0.] loss 389.02685546875\n",
      "3 67 output [0.5266917] target [3.] loss 417.14666748046875\n",
      "3 68 output [0.04919521] target [2.] loss 400.7473449707031\n",
      "3 69 output [2.0712245] target [1.] loss 377.2599182128906\n",
      "3 70 output [0.9351749] target [2.] loss 372.1387939453125\n",
      "3 71 output [1.1909783] target [0.] loss 407.1815185546875\n",
      "3 72 output [1.1928601] target [3.] loss 354.2932434082031\n",
      "Training [20%]\tLoss: 354.2932 Elapsed 1216.052336 total elapsed 3073.790963\n",
      "4 0 output [1.0000328] target [0.] loss 365.20404052734375\n",
      "4 1 output [1.1929346] target [1.] loss 406.5381164550781\n",
      "4 2 output [0.93997705] target [0.] loss 376.496337890625\n",
      "4 3 output [1.1285093] target [2.] loss 417.342041015625\n",
      "4 4 output [0.7910244] target [0.] loss 405.7646179199219\n",
      "4 5 output [1.7017541] target [0.] loss 387.15631103515625\n",
      "4 6 output [1.069203] target [3.] loss 326.1199645996094\n",
      "4 7 output [1.7089795] target [0.] loss 322.94964599609375\n",
      "4 8 output [1.8657346] target [2.] loss 355.09466552734375\n",
      "4 9 output [1.1750023] target [1.] loss 378.1037902832031\n",
      "4 10 output [1.0257983] target [1.] loss 300.2392578125\n",
      "4 11 output [1.189792] target [1.] loss 377.6741943359375\n",
      "4 12 output [1.191501] target [2.] loss 386.71142578125\n",
      "4 13 output [1.1971254] target [2.] loss 352.9888916015625\n",
      "4 14 output [0.5136482] target [2.] loss 362.02093505859375\n",
      "4 15 output [0.9669409] target [2.] loss 450.5706787109375\n",
      "4 16 output [1.8541541] target [3.] loss 328.5985107421875\n",
      "4 17 output [1.0919961] target [0.] loss 372.00067138671875\n",
      "4 18 output [0.02232069] target [0.] loss 334.5923156738281\n",
      "4 19 output [1.1978657] target [0.] loss 320.9287109375\n",
      "4 20 output [1.1854829] target [0.] loss 412.3346252441406\n",
      "4 21 output [1.1933088] target [1.] loss 371.79986572265625\n",
      "4 22 output [1.475646] target [0.] loss 356.00006103515625\n",
      "4 23 output [1.1980641] target [0.] loss 446.0084228515625\n",
      "4 24 output [1.1975515] target [0.] loss 318.91900634765625\n",
      "4 25 output [1.1932229] target [0.] loss 346.3572998046875\n",
      "4 26 output [1.0502615] target [0.] loss 388.63043212890625\n",
      "4 27 output [1.9059551] target [0.] loss 348.0050048828125\n",
      "4 28 output [1.2001454] target [2.] loss 409.7344665527344\n",
      "4 29 output [1.1974883] target [1.] loss 372.6268005371094\n",
      "4 30 output [1.2009288] target [4.] loss 366.83673095703125\n",
      "4 31 output [1.201067] target [0.] loss 349.3038635253906\n",
      "4 32 output [0.89013344] target [3.] loss 365.0244445800781\n",
      "4 33 output [1.201109] target [1.] loss 403.95111083984375\n",
      "4 34 output [1.9520178] target [0.] loss 443.674072265625\n",
      "4 35 output [1.3832704] target [3.] loss 404.5714416503906\n",
      "4 36 output [0.88283104] target [2.] loss 368.391357421875\n",
      "4 37 output [0.73438007] target [2.] loss 352.7490539550781\n",
      "4 38 output [1.0508581] target [0.] loss 346.5626220703125\n",
      "4 39 output [0.99601376] target [3.] loss 429.46063232421875\n",
      "4 40 output [2.4575095] target [3.] loss 382.27496337890625\n",
      "4 41 output [1.9634852] target [2.] loss 310.30548095703125\n",
      "4 42 output [1.1906093] target [2.] loss 361.78936767578125\n",
      "4 43 output [1.8667808] target [0.] loss 317.7489013671875\n",
      "4 44 output [1.0203522] target [2.] loss 370.1533508300781\n",
      "4 45 output [1.2004257] target [3.] loss 402.26708984375\n",
      "4 46 output [1.1705338] target [2.] loss 356.6173095703125\n",
      "4 47 output [1.1727237] target [1.] loss 389.59808349609375\n",
      "4 48 output [0.36303884] target [1.] loss 369.5885009765625\n",
      "4 49 output [1.1979067] target [0.] loss 353.34075927734375\n",
      "4 50 output [1.1943704] target [1.] loss 344.95416259765625\n",
      "4 51 output [1.084977] target [1.] loss 333.3572998046875\n",
      "4 52 output [2.0552254] target [0.] loss 377.06866455078125\n",
      "4 53 output [1.1881108] target [0.] loss 279.93072509765625\n",
      "4 54 output [1.1973139] target [1.] loss 400.27081298828125\n",
      "4 55 output [1.197217] target [2.] loss 414.52056884765625\n",
      "4 56 output [1.8653514] target [1.] loss 402.8286437988281\n",
      "4 57 output [0.9169452] target [0.] loss 325.8162841796875\n",
      "4 58 output [1.0314826] target [1.] loss 442.81182861328125\n",
      "4 59 output [1.1959692] target [1.] loss 402.326171875\n",
      "4 60 output [1.1904677] target [0.] loss 309.61102294921875\n",
      "4 61 output [1.761018] target [2.] loss 342.6136779785156\n",
      "4 62 output [1.1929384] target [1.] loss 335.493408203125\n",
      "4 63 output [1.049959] target [3.] loss 354.83209228515625\n",
      "4 64 output [0.29929933] target [0.] loss 352.4648742675781\n",
      "4 65 output [1.1674545] target [4.] loss 422.07281494140625\n",
      "4 66 output [1.1835076] target [2.] loss 341.767333984375\n",
      "4 67 output [0.99849933] target [1.] loss 381.905029296875\n",
      "4 68 output [1.1903367] target [2.] loss 419.3404235839844\n",
      "4 69 output [1.1901035] target [2.] loss 344.9888610839844\n",
      "4 70 output [1.1664973] target [0.] loss 350.478759765625\n",
      "4 71 output [1.1896961] target [3.] loss 362.68212890625\n",
      "4 72 output [1.9607008] target [2.] loss 309.2725830078125\n",
      "Training [25%]\tLoss: 309.2726 Elapsed 1530.691128 total elapsed 4604.482091\n",
      "5 0 output [2.3835888] target [2.] loss 361.02166748046875\n",
      "5 1 output [1.104148] target [2.] loss 365.2633361816406\n",
      "5 2 output [1.2946607] target [0.] loss 373.2619934082031\n",
      "5 3 output [0.98039174] target [2.] loss 462.56549072265625\n",
      "5 4 output [1.1252012] target [1.] loss 364.77642822265625\n",
      "5 5 output [1.4983273] target [1.] loss 312.1674499511719\n",
      "5 6 output [0.4701003] target [1.] loss 383.389892578125\n",
      "5 7 output [1.017109] target [1.] loss 394.37115478515625\n",
      "5 8 output [0.13386491] target [0.] loss 432.36773681640625\n",
      "5 9 output [1.188303] target [1.] loss 393.9906005859375\n",
      "5 10 output [1.5364884] target [0.] loss 324.7418212890625\n",
      "5 11 output [0.5020637] target [3.] loss 341.37945556640625\n",
      "5 12 output [1.4108171] target [1.] loss 386.0519104003906\n",
      "5 13 output [0.64269567] target [1.] loss 437.84796142578125\n",
      "5 14 output [1.8725708] target [2.] loss 407.66839599609375\n",
      "5 15 output [0.44038415] target [1.] loss 365.55682373046875\n",
      "5 16 output [1.1898756] target [0.] loss 345.8387451171875\n",
      "5 17 output [1.1395591] target [0.] loss 348.8531799316406\n",
      "5 18 output [1.1920445] target [1.] loss 430.2135925292969\n",
      "5 19 output [1.5404681] target [2.] loss 264.28070068359375\n",
      "5 20 output [1.1920538] target [1.] loss 333.5382080078125\n",
      "5 21 output [1.3764503] target [1.] loss 353.38482666015625\n",
      "5 22 output [0.85142255] target [1.] loss 378.79730224609375\n",
      "5 23 output [0.9882304] target [0.] loss 334.4893798828125\n",
      "5 24 output [1.1921287] target [1.] loss 355.8190612792969\n",
      "5 25 output [0.27977577] target [2.] loss 406.6949462890625\n",
      "5 26 output [1.7992218] target [1.] loss 362.6108093261719\n",
      "5 27 output [0.01538552] target [2.] loss 347.18048095703125\n",
      "5 28 output [1.0956771] target [1.] loss 375.4201354980469\n",
      "5 29 output [2.0718281] target [3.] loss 348.09478759765625\n",
      "5 30 output [1.1243622] target [0.] loss 414.29046630859375\n",
      "5 31 output [1.4334824] target [1.] loss 400.984619140625\n",
      "5 32 output [1.1874659] target [3.] loss 305.01385498046875\n",
      "5 33 output [1.1266087] target [1.] loss 295.509033203125\n",
      "5 34 output [1.0084403] target [1.] loss 370.49627685546875\n",
      "5 35 output [1.226377] target [2.] loss 391.67266845703125\n",
      "5 36 output [1.9134797] target [0.] loss 341.5133056640625\n",
      "5 37 output [0.5765976] target [2.] loss 394.829345703125\n",
      "5 38 output [0.50914043] target [0.] loss 349.4647216796875\n",
      "5 39 output [1.0826588] target [0.] loss 356.41229248046875\n",
      "5 40 output [0.6631527] target [1.] loss 376.5596923828125\n",
      "5 41 output [0.41793224] target [0.] loss 427.0126953125\n",
      "5 42 output [1.0679733] target [0.] loss 375.6078796386719\n",
      "5 43 output [2.573265] target [0.] loss 362.00860595703125\n",
      "5 44 output [1.849367] target [2.] loss 408.9970703125\n",
      "5 45 output [1.1914108] target [0.] loss 346.26776123046875\n",
      "5 46 output [1.190193] target [1.] loss 370.4137268066406\n",
      "5 47 output [1.1918138] target [1.] loss 334.9891052246094\n",
      "5 48 output [1.2533622] target [0.] loss 414.6510314941406\n",
      "5 49 output [1.1221035] target [1.] loss 367.97064208984375\n",
      "5 50 output [1.1638687] target [1.] loss 400.96966552734375\n",
      "5 51 output [1.1920955] target [0.] loss 344.46337890625\n",
      "5 52 output [1.1401293] target [1.] loss 352.83270263671875\n",
      "5 53 output [0.46975678] target [1.] loss 401.3568115234375\n",
      "5 54 output [1.1897286] target [3.] loss 265.4547119140625\n",
      "5 55 output [0.13045947] target [1.] loss 367.120361328125\n",
      "5 56 output [1.0664032] target [0.] loss 302.2147521972656\n",
      "5 57 output [1.1781261] target [1.] loss 346.3127136230469\n",
      "5 58 output [1.1861501] target [0.] loss 399.14312744140625\n",
      "5 59 output [2.1978502] target [0.] loss 368.3784484863281\n",
      "5 60 output [1.1861024] target [0.] loss 358.89324951171875\n",
      "5 61 output [0.72685665] target [1.] loss 358.15594482421875\n",
      "5 62 output [1.1857682] target [1.] loss 364.48614501953125\n",
      "5 63 output [1.1861495] target [0.] loss 335.03814697265625\n",
      "5 64 output [1.1381888] target [0.] loss 354.15277099609375\n",
      "5 65 output [1.1856066] target [1.] loss 329.9482116699219\n",
      "5 66 output [1.0780873] target [1.] loss 351.6007080078125\n",
      "5 67 output [1.18505] target [1.] loss 317.3631286621094\n",
      "5 68 output [1.1847069] target [0.] loss 328.91949462890625\n",
      "5 69 output [1.8983399] target [2.] loss 346.39923095703125\n",
      "5 70 output [1.1824038] target [0.] loss 353.92889404296875\n",
      "5 71 output [1.1839509] target [0.] loss 349.0846862792969\n",
      "5 72 output [1.518472] target [1.] loss 427.06756591796875\n",
      "Training [30%]\tLoss: 427.0676 Elapsed 1840.032831 total elapsed 6444.514922\n",
      "6 0 output [1.1637809] target [2.] loss 329.80859375\n",
      "6 1 output [1.4614424] target [2.] loss 399.51019287109375\n",
      "6 2 output [2.1263175] target [0.] loss 337.1282958984375\n",
      "6 3 output [0.3702756] target [0.] loss 356.12396240234375\n",
      "6 4 output [1.1399612] target [1.] loss 396.93133544921875\n",
      "6 5 output [1.0266501] target [1.] loss 263.6865539550781\n",
      "6 6 output [1.1477282] target [1.] loss 318.49896240234375\n",
      "6 7 output [1.6244216] target [0.] loss 325.0528259277344\n",
      "6 8 output [1.1838692] target [1.] loss 381.74432373046875\n",
      "6 9 output [1.1829731] target [1.] loss 352.4742431640625\n",
      "6 10 output [1.1237345] target [1.] loss 331.40325927734375\n",
      "6 11 output [1.5895386] target [3.] loss 355.55389404296875\n",
      "6 12 output [1.4126002] target [0.] loss 343.766357421875\n",
      "6 13 output [1.1790718] target [1.] loss 402.58941650390625\n",
      "6 14 output [0.74179155] target [0.] loss 349.61126708984375\n",
      "6 15 output [1.1768728] target [1.] loss 353.5778503417969\n",
      "6 16 output [1.1789072] target [1.] loss 421.99798583984375\n",
      "6 17 output [0.6663111] target [1.] loss 381.3740234375\n",
      "6 18 output [1.1768926] target [1.] loss 286.9114990234375\n",
      "6 19 output [1.0776778] target [3.] loss 366.8795166015625\n",
      "6 20 output [1.1743274] target [0.] loss 400.97869873046875\n",
      "6 21 output [1.1288971] target [5.] loss 384.40264892578125\n",
      "6 22 output [1.105279] target [1.] loss 369.6573181152344\n",
      "6 23 output [1.1722881] target [2.] loss 308.8399658203125\n",
      "6 24 output [1.2901022] target [0.] loss 327.749267578125\n",
      "6 25 output [1.1650538] target [2.] loss 346.1723937988281\n",
      "6 26 output [1.116651] target [1.] loss 309.9168701171875\n",
      "6 27 output [0.80109346] target [2.] loss 337.16497802734375\n",
      "6 28 output [1.16848] target [0.] loss 348.1012268066406\n",
      "6 29 output [1.2570801] target [2.] loss 337.95458984375\n",
      "6 30 output [1.1691008] target [1.] loss 333.8880615234375\n",
      "6 31 output [1.6669096] target [0.] loss 389.26641845703125\n",
      "6 32 output [0.68926346] target [1.] loss 370.96240234375\n",
      "6 33 output [1.1393785] target [0.] loss 397.10723876953125\n",
      "6 34 output [1.0817715] target [1.] loss 282.82861328125\n",
      "6 35 output [1.1710515] target [3.] loss 373.61248779296875\n",
      "6 36 output [0.10183094] target [2.] loss 430.4189147949219\n",
      "6 37 output [1.1720186] target [2.] loss 330.46722412109375\n",
      "6 38 output [1.1626825] target [1.] loss 368.262939453125\n",
      "6 39 output [1.1142334] target [2.] loss 380.8741455078125\n",
      "6 40 output [1.1635592] target [0.] loss 322.015869140625\n",
      "6 41 output [1.1603879] target [1.] loss 375.2359313964844\n",
      "6 42 output [1.175861] target [1.] loss 380.5401611328125\n",
      "6 43 output [0.4673903] target [0.] loss 327.84814453125\n",
      "6 44 output [0.7063546] target [2.] loss 345.3765869140625\n",
      "6 45 output [1.4628212] target [1.] loss 364.940673828125\n",
      "6 46 output [1.8840425] target [0.] loss 392.1206970214844\n",
      "6 47 output [0.87971747] target [0.] loss 344.1680603027344\n",
      "6 48 output [1.1774621] target [0.] loss 386.6032409667969\n",
      "6 49 output [0.8705921] target [1.] loss 316.61236572265625\n",
      "6 50 output [1.8631334] target [3.] loss 389.7546081542969\n",
      "6 51 output [1.1780732] target [3.] loss 416.734130859375\n",
      "6 52 output [1.2871842] target [0.] loss 281.457763671875\n",
      "6 53 output [1.1778218] target [1.] loss 300.1128234863281\n",
      "6 54 output [0.2102173] target [1.] loss 345.48516845703125\n",
      "6 55 output [1.1765201] target [1.] loss 312.3950500488281\n",
      "6 56 output [1.1753093] target [1.] loss 399.5208740234375\n",
      "6 57 output [1.1277559] target [3.] loss 355.3274841308594\n",
      "6 58 output [1.1670272] target [2.] loss 365.24560546875\n",
      "6 59 output [1.0912604] target [0.] loss 353.8414306640625\n",
      "6 60 output [1.1781611] target [2.] loss 424.13006591796875\n",
      "6 61 output [1.142642] target [2.] loss 374.8013916015625\n",
      "6 62 output [1.1801107] target [1.] loss 379.3443908691406\n",
      "6 63 output [0.01193628] target [1.] loss 265.07171630859375\n",
      "6 64 output [0.15455654] target [2.] loss 334.90081787109375\n",
      "6 65 output [0.8659718] target [1.] loss 382.7529296875\n",
      "6 66 output [1.0447958] target [0.] loss 320.3304748535156\n",
      "6 67 output [0.6011919] target [2.] loss 360.13323974609375\n",
      "6 68 output [1.1851103] target [1.] loss 329.609619140625\n",
      "6 69 output [1.1859132] target [1.] loss 298.5275573730469\n",
      "6 70 output [1.1859965] target [1.] loss 387.4651794433594\n",
      "6 71 output [1.1863856] target [0.] loss 311.682373046875\n",
      "6 72 output [0.28104252] target [3.] loss 345.6535339355469\n",
      "Training [35%]\tLoss: 345.6535 Elapsed 2122.991852 total elapsed 8567.506774000001\n",
      "7 0 output [1.1472342] target [0.] loss 335.5430603027344\n",
      "7 1 output [1.0676806] target [1.] loss 279.8276672363281\n",
      "7 2 output [1.0766783] target [3.] loss 308.17352294921875\n",
      "7 3 output [1.0314775] target [1.] loss 302.4705810546875\n",
      "7 4 output [1.1813953] target [0.] loss 365.9096374511719\n",
      "7 5 output [1.1768386] target [2.] loss 368.4705505371094\n",
      "7 6 output [1.1812432] target [1.] loss 365.30218505859375\n",
      "7 7 output [1.1759516] target [1.] loss 392.807861328125\n",
      "7 8 output [1.1794072] target [1.] loss 344.14776611328125\n",
      "7 9 output [0.43706727] target [1.] loss 390.4400329589844\n",
      "7 10 output [1.7366134] target [1.] loss 311.8439636230469\n",
      "7 11 output [1.1769481] target [1.] loss 318.9034423828125\n",
      "7 12 output [1.2518774] target [1.] loss 395.42645263671875\n",
      "7 13 output [0.25755844] target [1.] loss 378.46221923828125\n",
      "7 14 output [1.1036053] target [2.] loss 354.27484130859375\n",
      "7 15 output [0.5443982] target [0.] loss 301.50677490234375\n",
      "7 16 output [1.1714126] target [5.] loss 404.2889404296875\n",
      "7 17 output [0.9445349] target [3.] loss 391.489013671875\n",
      "7 18 output [1.1701388] target [0.] loss 295.2376708984375\n",
      "7 19 output [0.62041444] target [2.] loss 322.04412841796875\n",
      "7 20 output [1.1712009] target [0.] loss 347.7658996582031\n",
      "7 21 output [1.1707628] target [2.] loss 345.2388916015625\n",
      "7 22 output [0.498256] target [0.] loss 343.2115783691406\n",
      "7 23 output [1.0635877] target [3.] loss 320.4533386230469\n",
      "7 24 output [1.1507175] target [1.] loss 363.93414306640625\n",
      "7 25 output [1.1645613] target [1.] loss 417.5193786621094\n",
      "7 26 output [1.1680474] target [3.] loss 327.1443786621094\n",
      "7 27 output [1.7188158] target [0.] loss 322.3307800292969\n",
      "7 28 output [0.83711547] target [0.] loss 320.53326416015625\n",
      "7 29 output [1.7927744] target [1.] loss 359.03497314453125\n",
      "7 30 output [0.50709766] target [0.] loss 349.65301513671875\n",
      "7 31 output [0.30529886] target [2.] loss 410.2784118652344\n",
      "7 32 output [1.0461814] target [3.] loss 397.955810546875\n",
      "7 33 output [0.874648] target [0.] loss 388.0975036621094\n",
      "7 34 output [0.8342884] target [0.] loss 312.25592041015625\n",
      "7 35 output [1.1626786] target [0.] loss 329.7452697753906\n",
      "7 36 output [1.3210013] target [2.] loss 318.7392883300781\n",
      "7 37 output [0.2544418] target [2.] loss 393.9671630859375\n",
      "7 38 output [0.62939584] target [0.] loss 404.42620849609375\n",
      "7 39 output [2.3027244] target [1.] loss 369.3330078125\n",
      "7 40 output [1.1595702] target [0.] loss 340.8253173828125\n",
      "7 41 output [1.5750865] target [2.] loss 339.6595764160156\n",
      "7 42 output [1.2965803] target [0.] loss 371.17767333984375\n",
      "7 43 output [1.1600384] target [3.] loss 392.4388732910156\n",
      "7 44 output [1.1600108] target [1.] loss 402.60638427734375\n",
      "7 45 output [0.75784475] target [1.] loss 415.63018798828125\n",
      "7 46 output [0.5627421] target [1.] loss 331.93548583984375\n",
      "7 47 output [1.0632609] target [3.] loss 423.43621826171875\n",
      "7 48 output [1.0837241] target [0.] loss 429.0369873046875\n",
      "7 49 output [1.1646777] target [2.] loss 331.85089111328125\n",
      "7 50 output [1.1638418] target [1.] loss 385.56884765625\n",
      "7 51 output [1.1672647] target [0.] loss 329.04998779296875\n",
      "7 52 output [1.1667349] target [1.] loss 355.2127990722656\n",
      "7 53 output [1.1651639] target [2.] loss 440.811767578125\n",
      "7 54 output [0.21935886] target [1.] loss 341.2274475097656\n",
      "7 55 output [0.57792497] target [4.] loss 336.738037109375\n",
      "7 56 output [1.1710129] target [0.] loss 325.5842590332031\n",
      "7 57 output [1.0219884] target [2.] loss 386.7226867675781\n",
      "7 58 output [1.1747361] target [1.] loss 355.7403564453125\n",
      "7 59 output [1.5128787] target [0.] loss 324.8149108886719\n",
      "7 60 output [1.1763412] target [2.] loss 342.813720703125\n",
      "7 61 output [1.1772891] target [3.] loss 317.0892639160156\n",
      "7 62 output [1.6671689] target [1.] loss 381.56744384765625\n",
      "7 63 output [1.1792848] target [2.] loss 386.39837646484375\n",
      "7 64 output [1.1791766] target [0.] loss 351.40228271484375\n",
      "7 65 output [1.1774735] target [1.] loss 409.19476318359375\n",
      "7 66 output [1.1718432] target [1.] loss 317.04345703125\n",
      "7 67 output [1.4238877] target [2.] loss 355.5093994140625\n",
      "7 68 output [1.181008] target [1.] loss 309.43145751953125\n",
      "7 69 output [0.24972023] target [2.] loss 293.59881591796875\n",
      "7 70 output [1.180167] target [2.] loss 362.9281005859375\n",
      "7 71 output [1.0836356] target [2.] loss 356.2447814941406\n",
      "7 72 output [1.1695471] target [1.] loss 300.10784912109375\n",
      "Training [40%]\tLoss: 300.1078 Elapsed 2404.951414 total elapsed 10972.458188\n",
      "8 0 output [1.178286] target [1.] loss 345.2652587890625\n",
      "8 1 output [1.1775486] target [0.] loss 333.5013122558594\n",
      "8 2 output [1.1464761] target [2.] loss 288.51312255859375\n",
      "8 3 output [0.38046947] target [0.] loss 347.75531005859375\n",
      "8 4 output [0.26107296] target [4.] loss 408.3646240234375\n",
      "8 5 output [0.13886258] target [1.] loss 307.3896484375\n",
      "8 6 output [1.1753924] target [1.] loss 387.259521484375\n",
      "8 7 output [1.0485795] target [2.] loss 348.2227783203125\n",
      "8 8 output [1.046233] target [0.] loss 281.181396484375\n",
      "8 9 output [0.7610001] target [0.] loss 343.07794189453125\n",
      "8 10 output [0.60754275] target [0.] loss 390.5213623046875\n",
      "8 11 output [0.7713363] target [3.] loss 370.120361328125\n",
      "8 12 output [0.45602766] target [2.] loss 405.71429443359375\n",
      "8 13 output [0.58924145] target [3.] loss 332.92999267578125\n",
      "8 14 output [1.1743929] target [3.] loss 393.4123840332031\n",
      "8 15 output [1.1226963] target [1.] loss 321.9117126464844\n",
      "8 16 output [1.1665576] target [2.] loss 373.71466064453125\n",
      "8 17 output [1.1625961] target [3.] loss 288.5715026855469\n",
      "8 18 output [1.1773586] target [4.] loss 325.0235595703125\n",
      "8 19 output [1.1439383] target [1.] loss 413.9597473144531\n",
      "8 20 output [1.1786411] target [2.] loss 309.2041015625\n",
      "8 21 output [1.1780465] target [1.] loss 349.0127258300781\n",
      "8 22 output [1.1789237] target [1.] loss 343.37811279296875\n",
      "8 23 output [0.45965865] target [0.] loss 343.73101806640625\n",
      "8 24 output [1.1792082] target [1.] loss 317.014892578125\n",
      "8 25 output [1.1793036] target [0.] loss 282.2891845703125\n",
      "8 26 output [1.1791265] target [2.] loss 403.0628662109375\n",
      "8 27 output [1.126164] target [1.] loss 350.6377868652344\n",
      "8 28 output [1.1789515] target [0.] loss 297.9103088378906\n",
      "8 29 output [1.1574571] target [2.] loss 357.445068359375\n",
      "8 30 output [1.5919294] target [2.] loss 333.31787109375\n",
      "8 31 output [0.87426764] target [0.] loss 350.79168701171875\n",
      "8 32 output [0.29228073] target [0.] loss 312.11920166015625\n",
      "8 33 output [1.1797395] target [1.] loss 291.7891845703125\n",
      "8 34 output [1.1807176] target [0.] loss 395.4762268066406\n",
      "8 35 output [1.1601354] target [0.] loss 317.38055419921875\n",
      "8 36 output [1.1782126] target [0.] loss 300.55078125\n",
      "8 37 output [1.1305557] target [2.] loss 357.52032470703125\n",
      "8 38 output [1.1796441] target [1.] loss 317.82330322265625\n",
      "8 39 output [1.17769] target [0.] loss 358.7911376953125\n",
      "8 40 output [1.3306714] target [1.] loss 285.37994384765625\n",
      "8 41 output [1.149685] target [2.] loss 353.626708984375\n",
      "8 42 output [1.1772978] target [0.] loss 358.30389404296875\n",
      "8 43 output [1.0897226] target [0.] loss 366.433837890625\n",
      "8 44 output [1.1559623] target [0.] loss 310.3299560546875\n",
      "8 45 output [1.1732742] target [2.] loss 331.63671875\n",
      "8 46 output [1.1758236] target [0.] loss 326.9786376953125\n",
      "8 47 output [1.1745366] target [0.] loss 286.4410095214844\n",
      "8 48 output [1.1733686] target [3.] loss 352.808837890625\n",
      "8 49 output [1.6849062] target [2.] loss 348.93377685546875\n",
      "8 50 output [1.1717331] target [0.] loss 298.2669677734375\n",
      "8 51 output [1.1546698] target [1.] loss 366.27716064453125\n",
      "8 52 output [1.1720519] target [1.] loss 419.59527587890625\n",
      "8 53 output [1.1701479] target [3.] loss 354.029296875\n",
      "8 54 output [1.1695877] target [1.] loss 314.0830078125\n",
      "8 55 output [0.43278247] target [0.] loss 315.59356689453125\n",
      "8 56 output [0.93979186] target [1.] loss 443.399169921875\n",
      "8 57 output [1.1683388] target [3.] loss 387.07659912109375\n",
      "8 58 output [1.48855] target [0.] loss 328.68988037109375\n",
      "8 59 output [1.1621901] target [0.] loss 301.2545166015625\n",
      "8 60 output [1.1664493] target [2.] loss 388.5864562988281\n",
      "8 61 output [1.1697645] target [0.] loss 360.67498779296875\n",
      "8 62 output [1.5488603] target [2.] loss 350.79736328125\n",
      "8 63 output [1.4752432] target [0.] loss 396.47320556640625\n",
      "8 64 output [1.1698474] target [2.] loss 354.73931884765625\n",
      "8 65 output [1.1485658] target [2.] loss 318.8546142578125\n",
      "8 66 output [0.24314971] target [0.] loss 272.1893310546875\n",
      "8 67 output [1.1689985] target [0.] loss 344.14154052734375\n",
      "8 68 output [1.2509562] target [0.] loss 304.37158203125\n",
      "8 69 output [1.1614963] target [2.] loss 262.6643981933594\n",
      "8 70 output [1.1674125] target [0.] loss 317.0135803222656\n",
      "8 71 output [1.1659853] target [2.] loss 365.9237060546875\n",
      "8 72 output [1.1646916] target [3.] loss 342.2460632324219\n",
      "Training [45%]\tLoss: 342.2461 Elapsed 2691.391004 total elapsed 13663.849192000001\n",
      "9 0 output [1.1635139] target [1.] loss 363.2637023925781\n",
      "9 1 output [0.03917673] target [0.] loss 298.894287109375\n",
      "9 2 output [1.1451685] target [1.] loss 399.26348876953125\n",
      "9 3 output [1.1606408] target [2.] loss 324.0278015136719\n",
      "9 4 output [1.1594763] target [3.] loss 338.767822265625\n",
      "9 5 output [0.7628592] target [0.] loss 303.525634765625\n",
      "9 6 output [1.1591891] target [0.] loss 336.7705993652344\n",
      "9 7 output [0.8504548] target [0.] loss 290.6111755371094\n",
      "9 8 output [1.1598307] target [1.] loss 357.82916259765625\n",
      "9 9 output [1.1381612] target [3.] loss 336.92706298828125\n",
      "9 10 output [0.75745887] target [2.] loss 276.286376953125\n",
      "9 11 output [1.1587346] target [1.] loss 357.9324951171875\n",
      "9 12 output [0.8597264] target [0.] loss 292.9438781738281\n",
      "9 13 output [1.1543305] target [0.] loss 387.8966064453125\n",
      "9 14 output [1.7203844] target [1.] loss 340.15069580078125\n",
      "9 15 output [1.6717418] target [0.] loss 305.1479187011719\n",
      "9 16 output [1.1560998] target [2.] loss 361.9114990234375\n",
      "9 17 output [1.1557239] target [1.] loss 309.41070556640625\n",
      "9 18 output [0.72099113] target [0.] loss 318.811767578125\n",
      "9 19 output [1.1554424] target [1.] loss 353.16888427734375\n",
      "9 20 output [1.1645701] target [1.] loss 340.35302734375\n",
      "9 21 output [1.155289] target [2.] loss 432.59832763671875\n",
      "9 22 output [1.1551468] target [1.] loss 345.3306884765625\n",
      "9 23 output [1.1445389] target [1.] loss 336.40740966796875\n",
      "9 24 output [1.3895398] target [1.] loss 350.6336669921875\n",
      "9 25 output [1.0699381] target [0.] loss 344.91632080078125\n",
      "9 26 output [1.1583465] target [0.] loss 295.5190124511719\n",
      "9 27 output [1.1510481] target [1.] loss 390.8978271484375\n",
      "9 28 output [1.1599886] target [0.] loss 340.1408996582031\n",
      "9 29 output [1.1569421] target [2.] loss 310.8008728027344\n",
      "9 30 output [1.0904335] target [0.] loss 275.92254638671875\n",
      "9 31 output [1.1625333] target [1.] loss 330.08453369140625\n",
      "9 32 output [1.162937] target [1.] loss 384.37744140625\n",
      "9 33 output [1.0353332] target [1.] loss 304.5093078613281\n",
      "9 34 output [1.1637374] target [0.] loss 396.77899169921875\n",
      "9 35 output [1.1639159] target [1.] loss 258.86541748046875\n",
      "9 36 output [1.121036] target [0.] loss 306.16204833984375\n",
      "9 37 output [1.15323] target [1.] loss 338.60321044921875\n",
      "9 38 output [1.1627102] target [1.] loss 324.3081359863281\n",
      "9 39 output [1.1626322] target [0.] loss 397.0584716796875\n",
      "9 40 output [1.1631241] target [3.] loss 329.5906677246094\n",
      "9 41 output [1.1636019] target [5.] loss 325.7938232421875\n",
      "9 42 output [1.1633987] target [0.] loss 321.60089111328125\n",
      "9 43 output [1.8948054] target [2.] loss 342.963623046875\n",
      "9 44 output [1.1638589] target [1.] loss 319.61962890625\n",
      "9 45 output [1.1559858] target [2.] loss 403.39947509765625\n",
      "9 46 output [1.1651378] target [0.] loss 404.46075439453125\n",
      "9 47 output [1.1661521] target [0.] loss 319.1541748046875\n",
      "9 48 output [1.1670876] target [4.] loss 333.4843444824219\n",
      "9 49 output [1.1679125] target [0.] loss 293.50408935546875\n",
      "9 50 output [1.1682441] target [1.] loss 325.524658203125\n",
      "9 51 output [1.2344947] target [0.] loss 315.114990234375\n",
      "9 52 output [1.1675781] target [2.] loss 377.6994323730469\n",
      "9 53 output [0.22351357] target [1.] loss 294.51629638671875\n",
      "9 54 output [0.3969495] target [0.] loss 347.49169921875\n",
      "9 55 output [1.1576335] target [2.] loss 401.7596435546875\n",
      "9 56 output [1.7243232] target [0.] loss 404.91094970703125\n",
      "9 57 output [1.3456982] target [1.] loss 343.65057373046875\n",
      "9 58 output [1.67201] target [1.] loss 337.45025634765625\n",
      "9 59 output [1.167557] target [1.] loss 291.5950927734375\n",
      "9 60 output [0.46576226] target [1.] loss 332.1803894042969\n",
      "9 61 output [1.1392763] target [0.] loss 325.1050109863281\n",
      "9 62 output [1.167151] target [0.] loss 364.0681457519531\n",
      "9 63 output [0.90299666] target [0.] loss 340.7969665527344\n",
      "9 64 output [0.17800182] target [0.] loss 288.54132080078125\n",
      "9 65 output [1.1603744] target [0.] loss 316.337890625\n",
      "9 66 output [1.1299071] target [0.] loss 308.0789794921875\n",
      "9 67 output [1.4601791] target [1.] loss 297.1284484863281\n",
      "9 68 output [1.4945352] target [0.] loss 364.0840759277344\n",
      "9 69 output [1.0996689] target [1.] loss 368.5372619628906\n",
      "9 70 output [1.2924237] target [0.] loss 305.95660400390625\n",
      "9 71 output [1.1348782] target [1.] loss 372.5132751464844\n",
      "9 72 output [1.1615316] target [0.] loss 352.7709045410156\n",
      "Training [50%]\tLoss: 352.7709 Elapsed 2966.011706 total elapsed 16629.860898000003\n",
      "10 0 output [0.06226442] target [1.] loss 378.3276062011719\n",
      "10 1 output [1.1608181] target [1.] loss 306.6268310546875\n",
      "10 2 output [1.160794] target [1.] loss 290.2158203125\n",
      "10 3 output [1.1408824] target [0.] loss 361.793212890625\n",
      "10 4 output [1.1593425] target [3.] loss 269.1605529785156\n",
      "10 5 output [1.0586635] target [2.] loss 280.4444274902344\n",
      "10 6 output [1.1256164] target [1.] loss 336.45318603515625\n",
      "10 7 output [1.1581131] target [0.] loss 297.60687255859375\n",
      "10 8 output [1.1588514] target [3.] loss 353.6951904296875\n",
      "10 9 output [1.1583478] target [0.] loss 334.5128173828125\n",
      "10 10 output [1.8473036] target [2.] loss 353.90728759765625\n",
      "10 11 output [1.1567749] target [0.] loss 352.0946960449219\n",
      "10 12 output [1.1562035] target [0.] loss 320.1656188964844\n",
      "10 13 output [1.0905296] target [1.] loss 345.5171813964844\n",
      "10 14 output [1.1513027] target [0.] loss 354.4755859375\n",
      "10 15 output [1.1550176] target [0.] loss 330.5926513671875\n",
      "10 16 output [1.1293995] target [3.] loss 329.095458984375\n",
      "10 17 output [1.8771096] target [3.] loss 396.5855712890625\n",
      "10 18 output [1.1074873] target [1.] loss 345.7874755859375\n",
      "10 19 output [1.0965039] target [2.] loss 362.66241455078125\n",
      "10 20 output [1.1575348] target [1.] loss 369.3548583984375\n",
      "10 21 output [1.231123] target [0.] loss 332.10748291015625\n",
      "10 22 output [1.1598879] target [3.] loss 360.83074951171875\n",
      "10 23 output [0.3266117] target [0.] loss 298.8343505859375\n",
      "10 24 output [1.4397805] target [2.] loss 360.98651123046875\n",
      "10 25 output [1.1615858] target [0.] loss 310.6149597167969\n",
      "10 26 output [1.3038578] target [2.] loss 295.0376281738281\n",
      "10 27 output [1.1440054] target [0.] loss 347.80267333984375\n",
      "10 28 output [0.96270084] target [0.] loss 308.8382568359375\n",
      "10 29 output [1.167668] target [1.] loss 356.768310546875\n",
      "10 30 output [1.16808] target [4.] loss 288.08892822265625\n",
      "10 31 output [0.26705405] target [3.] loss 332.46453857421875\n",
      "10 32 output [1.1537838] target [2.] loss 328.0085144042969\n",
      "10 33 output [1.421046] target [2.] loss 366.98297119140625\n",
      "10 34 output [1.1691039] target [2.] loss 267.310302734375\n",
      "10 35 output [1.8384223] target [0.] loss 318.9702453613281\n",
      "10 36 output [1.1708639] target [3.] loss 317.0513916015625\n",
      "10 37 output [1.1714019] target [0.] loss 363.94256591796875\n",
      "10 38 output [1.133531] target [0.] loss 319.5443420410156\n",
      "10 39 output [1.1729978] target [0.] loss 331.828857421875\n",
      "10 40 output [1.1734619] target [0.] loss 327.7919921875\n",
      "10 41 output [1.1319777] target [1.] loss 290.5118408203125\n",
      "10 42 output [1.1732328] target [0.] loss 370.4507141113281\n",
      "10 43 output [1.1730888] target [0.] loss 364.6480712890625\n",
      "10 44 output [1.2506098] target [1.] loss 366.35009765625\n",
      "10 45 output [1.1724931] target [1.] loss 313.0891418457031\n",
      "10 46 output [1.1726761] target [0.] loss 364.92340087890625\n",
      "10 47 output [1.362126] target [0.] loss 350.5703125\n",
      "10 48 output [1.1729925] target [3.] loss 361.64971923828125\n",
      "10 49 output [1.1721115] target [2.] loss 313.40386962890625\n",
      "10 50 output [1.0987487] target [1.] loss 348.33544921875\n",
      "10 51 output [1.1722814] target [2.] loss 347.98345947265625\n",
      "10 52 output [0.35088253] target [1.] loss 400.33660888671875\n",
      "10 53 output [1.1718861] target [2.] loss 383.0985412597656\n",
      "10 54 output [1.1721277] target [0.] loss 370.96868896484375\n",
      "10 55 output [1.1694765] target [2.] loss 324.9956970214844\n",
      "10 56 output [1.1721079] target [0.] loss 326.93658447265625\n",
      "10 57 output [1.168417] target [2.] loss 319.40399169921875\n",
      "10 58 output [1.1706494] target [2.] loss 348.875\n",
      "10 59 output [0.7160222] target [1.] loss 323.8282165527344\n",
      "10 60 output [1.1704307] target [2.] loss 283.3730773925781\n",
      "10 61 output [1.2293749] target [1.] loss 354.6366882324219\n",
      "10 62 output [1.1698639] target [1.] loss 377.5076904296875\n",
      "10 63 output [0.1933111] target [0.] loss 358.6731872558594\n",
      "10 64 output [1.1707668] target [0.] loss 345.58843994140625\n",
      "10 65 output [0.75465643] target [0.] loss 327.9246826171875\n",
      "10 66 output [1.1213679] target [1.] loss 348.16033935546875\n",
      "10 67 output [1.1710776] target [0.] loss 316.24951171875\n",
      "10 68 output [0.04315475] target [0.] loss 327.49267578125\n",
      "10 69 output [1.1725647] target [0.] loss 353.6097412109375\n",
      "10 70 output [1.1725162] target [0.] loss 394.58636474609375\n",
      "10 71 output [1.1708573] target [2.] loss 389.81951904296875\n",
      "10 72 output [0.67688] target [1.] loss 325.7762756347656\n",
      "Training [55%]\tLoss: 325.7763 Elapsed 3260.586866 total elapsed 19890.447764000004\n",
      "11 0 output [1.0274552] target [2.] loss 323.0849609375\n",
      "11 1 output [1.1729617] target [1.] loss 388.12841796875\n",
      "11 2 output [1.1714497] target [0.] loss 357.915283203125\n",
      "11 3 output [1.1735039] target [3.] loss 290.9608459472656\n",
      "11 4 output [0.19906841] target [0.] loss 323.2230224609375\n",
      "11 5 output [1.1734164] target [1.] loss 373.78070068359375\n",
      "11 6 output [1.5997672] target [1.] loss 262.1293029785156\n",
      "11 7 output [1.1441948] target [0.] loss 314.5420227050781\n",
      "11 8 output [1.1701585] target [2.] loss 322.77166748046875\n",
      "11 9 output [1.1731528] target [0.] loss 315.6304931640625\n",
      "11 10 output [1.1725657] target [1.] loss 330.8333740234375\n",
      "11 11 output [1.1561384] target [3.] loss 344.8677978515625\n",
      "11 12 output [1.1706789] target [1.] loss 254.63084411621094\n",
      "11 13 output [1.1549288] target [2.] loss 303.0747985839844\n",
      "11 14 output [1.1686479] target [2.] loss 359.88348388671875\n",
      "11 15 output [1.1679702] target [1.] loss 370.6824951171875\n",
      "11 16 output [1.1583085] target [0.] loss 357.8568115234375\n",
      "11 17 output [1.1672789] target [1.] loss 316.5829162597656\n",
      "11 18 output [1.1651185] target [1.] loss 306.8806457519531\n",
      "11 19 output [1.1665447] target [1.] loss 293.4236145019531\n",
      "11 20 output [0.17255706] target [1.] loss 302.551025390625\n",
      "11 21 output [1.1644418] target [0.] loss 362.78253173828125\n",
      "11 22 output [1.1625434] target [1.] loss 362.9423522949219\n",
      "11 23 output [1.1625538] target [1.] loss 323.666748046875\n",
      "11 24 output [1.1628364] target [1.] loss 352.692138671875\n",
      "11 25 output [1.1625077] target [2.] loss 263.01483154296875\n",
      "11 26 output [1.357095] target [3.] loss 325.8681945800781\n",
      "11 27 output [1.6460853] target [3.] loss 318.2483825683594\n",
      "11 28 output [1.1605219] target [5.] loss 390.22882080078125\n",
      "11 29 output [1.1606588] target [2.] loss 284.633056640625\n",
      "11 30 output [1.1608659] target [2.] loss 372.218017578125\n",
      "11 31 output [0.507325] target [0.] loss 321.906494140625\n",
      "11 32 output [1.0836126] target [0.] loss 355.8529968261719\n",
      "11 33 output [0.8614551] target [2.] loss 353.9989929199219\n",
      "11 34 output [1.1631393] target [1.] loss 303.91143798828125\n",
      "11 35 output [0.41293386] target [0.] loss 333.4005126953125\n",
      "11 36 output [1.1621599] target [1.] loss 320.71197509765625\n",
      "11 37 output [1.3352742] target [2.] loss 348.089599609375\n",
      "11 38 output [1.1605841] target [5.] loss 357.353515625\n",
      "11 39 output [1.0879371] target [1.] loss 303.1156005859375\n",
      "11 40 output [1.1649605] target [1.] loss 321.1966552734375\n",
      "11 41 output [1.1650584] target [1.] loss 357.0483093261719\n",
      "11 42 output [1.165241] target [0.] loss 358.59600830078125\n",
      "11 43 output [0.16166863] target [0.] loss 329.9779052734375\n",
      "11 44 output [1.1656904] target [0.] loss 362.55926513671875\n",
      "11 45 output [1.5642874] target [0.] loss 289.81689453125\n",
      "11 46 output [1.1657109] target [3.] loss 371.06439208984375\n",
      "11 47 output [1.1655318] target [1.] loss 353.68792724609375\n",
      "11 48 output [1.1651922] target [1.] loss 308.6976318359375\n",
      "11 49 output [1.1648387] target [1.] loss 332.2259521484375\n",
      "11 50 output [1.1643223] target [0.] loss 357.7783508300781\n",
      "11 51 output [1.1642094] target [1.] loss 355.51470947265625\n",
      "11 52 output [1.6602448] target [0.] loss 337.68634033203125\n",
      "11 53 output [1.1647158] target [4.] loss 345.4901428222656\n",
      "11 54 output [1.1653715] target [2.] loss 289.4898681640625\n",
      "11 55 output [1.1660291] target [1.] loss 282.9154052734375\n",
      "11 56 output [1.1578622] target [0.] loss 259.67437744140625\n",
      "11 57 output [1.165951] target [2.] loss 362.3833312988281\n",
      "11 58 output [1.165728] target [0.] loss 290.016357421875\n",
      "11 59 output [1.1601983] target [0.] loss 287.78515625\n",
      "11 60 output [1.1630933] target [0.] loss 356.4720458984375\n",
      "11 61 output [1.1536349] target [0.] loss 339.0426940917969\n",
      "11 62 output [1.3008113] target [2.] loss 310.08209228515625\n",
      "11 63 output [1.1634939] target [1.] loss 338.2918701171875\n",
      "11 64 output [1.5091264] target [1.] loss 298.041748046875\n",
      "11 65 output [1.1607873] target [1.] loss 335.617431640625\n",
      "11 66 output [1.1158065] target [2.] loss 329.80859375\n",
      "11 67 output [0.9944439] target [2.] loss 346.699462890625\n",
      "11 68 output [1.1646265] target [0.] loss 342.967529296875\n",
      "11 69 output [1.1646186] target [2.] loss 387.6885986328125\n",
      "11 70 output [1.1656052] target [1.] loss 337.005126953125\n",
      "11 71 output [1.1661434] target [5.] loss 331.16766357421875\n",
      "11 72 output [1.1661606] target [2.] loss 307.228515625\n",
      "Training [60%]\tLoss: 307.2285 Elapsed 3573.537837 total elapsed 23463.985601000004\n",
      "12 0 output [1.1944276] target [1.] loss 314.971435546875\n",
      "12 1 output [1.1662469] target [2.] loss 393.7557373046875\n",
      "12 2 output [1.1664941] target [2.] loss 322.1590881347656\n",
      "12 3 output [0.83706653] target [0.] loss 292.94281005859375\n",
      "12 4 output [1.2338715] target [2.] loss 374.441650390625\n",
      "12 5 output [1.1675116] target [1.] loss 359.33251953125\n",
      "12 6 output [1.6954157] target [1.] loss 288.4652404785156\n",
      "12 7 output [1.1530304] target [2.] loss 293.5365295410156\n",
      "12 8 output [1.1664162] target [2.] loss 269.2386474609375\n",
      "12 9 output [1.2497818] target [1.] loss 351.12371826171875\n",
      "12 10 output [0.0307712] target [1.] loss 342.9184875488281\n",
      "12 11 output [1.1687919] target [0.] loss 341.62725830078125\n",
      "12 12 output [1.1573384] target [2.] loss 366.9056396484375\n",
      "12 13 output [1.1697384] target [1.] loss 315.3996887207031\n",
      "12 14 output [1.1640745] target [1.] loss 386.13653564453125\n",
      "12 15 output [1.1697627] target [4.] loss 374.03289794921875\n",
      "12 16 output [0.28592807] target [1.] loss 322.11505126953125\n",
      "12 17 output [1.1713815] target [0.] loss 358.43829345703125\n",
      "12 18 output [0.27017027] target [3.] loss 324.5167236328125\n",
      "12 19 output [0.97644925] target [0.] loss 298.3517761230469\n",
      "12 20 output [1.1724331] target [2.] loss 316.43450927734375\n",
      "12 21 output [1.5810862] target [1.] loss 281.89312744140625\n",
      "12 22 output [0.93539876] target [0.] loss 367.96331787109375\n",
      "12 23 output [1.171775] target [0.] loss 351.2447509765625\n",
      "12 24 output [1.1477842] target [0.] loss 333.7596740722656\n",
      "12 25 output [1.5092976] target [1.] loss 326.0220947265625\n",
      "12 26 output [1.1706443] target [1.] loss 308.65789794921875\n",
      "12 27 output [1.1698012] target [0.] loss 339.69464111328125\n",
      "12 28 output [1.1688243] target [0.] loss 373.5364990234375\n",
      "12 29 output [1.1685234] target [3.] loss 375.6382141113281\n",
      "12 30 output [1.1539596] target [1.] loss 332.396484375\n",
      "12 31 output [1.1682446] target [0.] loss 315.7945251464844\n",
      "12 32 output [1.3905039] target [1.] loss 330.3861083984375\n",
      "12 33 output [1.1677275] target [2.] loss 386.3289794921875\n",
      "12 34 output [1.1243485] target [0.] loss 353.07073974609375\n",
      "12 35 output [0.90329576] target [1.] loss 349.3673095703125\n",
      "12 36 output [1.1662006] target [1.] loss 269.4908752441406\n",
      "12 37 output [1.1560354] target [0.] loss 348.2538757324219\n",
      "12 38 output [1.1576834] target [4.] loss 296.2596435546875\n",
      "12 39 output [1.1230397] target [0.] loss 369.643798828125\n",
      "12 40 output [1.1117729] target [0.] loss 352.023681640625\n",
      "12 41 output [1.1641995] target [1.] loss 327.37481689453125\n",
      "12 42 output [1.15647] target [1.] loss 302.1851806640625\n",
      "12 43 output [1.1629956] target [4.] loss 288.8764953613281\n",
      "12 44 output [0.268267] target [1.] loss 357.1094665527344\n",
      "12 45 output [1.1599827] target [1.] loss 335.6820983886719\n",
      "12 46 output [1.1373662] target [1.] loss 324.94451904296875\n",
      "12 47 output [1.1607295] target [0.] loss 285.6485595703125\n",
      "12 48 output [1.1311538] target [1.] loss 379.3785705566406\n",
      "12 49 output [1.1594787] target [5.] loss 330.24090576171875\n",
      "12 50 output [1.159258] target [0.] loss 278.91998291015625\n",
      "12 51 output [1.1585662] target [0.] loss 304.2288818359375\n",
      "12 52 output [0.88633895] target [1.] loss 325.4969177246094\n",
      "12 53 output [1.157889] target [0.] loss 324.36041259765625\n",
      "12 54 output [1.1348045] target [1.] loss 320.1346435546875\n",
      "12 55 output [1.1579723] target [0.] loss 321.45721435546875\n",
      "12 56 output [1.1578286] target [2.] loss 318.2708740234375\n",
      "12 57 output [1.1583532] target [2.] loss 315.9165954589844\n",
      "12 58 output [0.40069833] target [0.] loss 285.8367919921875\n",
      "12 59 output [1.1590115] target [1.] loss 298.61590576171875\n",
      "12 60 output [1.0245814] target [1.] loss 395.84320068359375\n",
      "12 61 output [1.1567494] target [1.] loss 317.2830810546875\n",
      "12 62 output [0.86678183] target [0.] loss 314.1100769042969\n",
      "12 63 output [1.156947] target [3.] loss 324.3386535644531\n",
      "12 64 output [1.1577125] target [1.] loss 316.71337890625\n",
      "12 65 output [1.1577191] target [0.] loss 330.3614807128906\n",
      "12 66 output [1.1578145] target [0.] loss 305.8496398925781\n",
      "12 67 output [1.1576487] target [1.] loss 294.8109130859375\n",
      "12 68 output [1.1576152] target [1.] loss 343.1028137207031\n",
      "12 69 output [1.1475024] target [0.] loss 258.6564025878906\n",
      "12 70 output [1.0810127] target [3.] loss 281.0640869140625\n",
      "12 71 output [1.1568801] target [5.] loss 372.5089111328125\n",
      "12 72 output [1.5692656] target [0.] loss 269.97308349609375\n",
      "Training [65%]\tLoss: 269.9731 Elapsed 3857.257416 total elapsed 27321.243017000004\n",
      "13 0 output [1.153713] target [0.] loss 387.7239990234375\n",
      "13 1 output [1.1386826] target [0.] loss 301.4072265625\n",
      "13 2 output [1.1568509] target [1.] loss 314.0301513671875\n",
      "13 3 output [1.1559607] target [1.] loss 295.28607177734375\n",
      "13 4 output [0.8034303] target [0.] loss 389.852294921875\n",
      "13 5 output [1.1559603] target [2.] loss 328.7236328125\n",
      "13 6 output [1.1559031] target [1.] loss 314.84503173828125\n",
      "13 7 output [1.6863174] target [1.] loss 324.03863525390625\n",
      "13 8 output [1.1556733] target [1.] loss 334.0022277832031\n",
      "13 9 output [1.1053929] target [0.] loss 354.216552734375\n",
      "13 10 output [1.7129047] target [0.] loss 294.6783752441406\n",
      "13 11 output [1.0930339] target [2.] loss 363.6501159667969\n",
      "13 12 output [1.1389207] target [1.] loss 306.7365417480469\n",
      "13 13 output [0.966401] target [3.] loss 334.713623046875\n",
      "13 14 output [0.28111655] target [0.] loss 330.575439453125\n",
      "13 15 output [1.1583416] target [2.] loss 368.38397216796875\n",
      "13 16 output [1.1064979] target [1.] loss 350.1676330566406\n",
      "13 17 output [1.1587327] target [2.] loss 399.9797668457031\n",
      "13 18 output [0.11979181] target [1.] loss 300.01251220703125\n",
      "13 19 output [1.3736295] target [2.] loss 375.8175964355469\n",
      "13 20 output [1.1623558] target [1.] loss 327.4283447265625\n",
      "13 21 output [1.162848] target [1.] loss 294.4360046386719\n",
      "13 22 output [0.85528326] target [1.] loss 276.69976806640625\n",
      "13 23 output [0.9836005] target [1.] loss 357.929931640625\n",
      "13 24 output [0.98463726] target [1.] loss 322.66815185546875\n",
      "13 25 output [0.20097351] target [3.] loss 313.4883728027344\n",
      "13 26 output [1.1602226] target [1.] loss 357.22735595703125\n",
      "13 27 output [1.1504865] target [0.] loss 349.67962646484375\n",
      "13 28 output [0.85151803] target [1.] loss 309.1212158203125\n",
      "13 29 output [1.1850469] target [0.] loss 367.8565673828125\n",
      "13 30 output [1.1645749] target [2.] loss 327.8343505859375\n",
      "13 31 output [1.1646415] target [0.] loss 337.39459228515625\n",
      "13 32 output [1.1637238] target [0.] loss 305.6458435058594\n",
      "13 33 output [0.12947555] target [1.] loss 362.7530517578125\n",
      "13 34 output [1.1650488] target [1.] loss 298.39825439453125\n",
      "13 35 output [1.1647823] target [0.] loss 318.0053405761719\n",
      "13 36 output [1.0625494] target [3.] loss 328.10809326171875\n",
      "13 37 output [1.1627488] target [3.] loss 331.8599853515625\n",
      "13 38 output [0.25442508] target [0.] loss 328.7592468261719\n",
      "13 39 output [1.2292404] target [3.] loss 325.799560546875\n",
      "13 40 output [0.48363864] target [1.] loss 336.6449890136719\n",
      "13 41 output [1.1546724] target [0.] loss 349.376220703125\n",
      "13 42 output [1.1452193] target [2.] loss 319.4674072265625\n",
      "13 43 output [1.1015836] target [2.] loss 312.30657958984375\n",
      "13 44 output [1.1584435] target [1.] loss 374.3695068359375\n",
      "13 45 output [1.1316378] target [2.] loss 328.9759521484375\n",
      "13 46 output [1.1580753] target [3.] loss 338.10052490234375\n",
      "13 47 output [0.54739404] target [2.] loss 290.58636474609375\n",
      "13 48 output [1.1573967] target [2.] loss 326.8505859375\n",
      "13 49 output [1.1548464] target [3.] loss 362.2485656738281\n",
      "13 50 output [1.1568635] target [2.] loss 323.4658203125\n",
      "13 51 output [1.1567436] target [0.] loss 288.1684875488281\n",
      "13 52 output [1.156126] target [2.] loss 393.50537109375\n",
      "13 53 output [1.4283179] target [1.] loss 332.78387451171875\n",
      "13 54 output [1.155496] target [2.] loss 308.5395202636719\n",
      "13 55 output [1.1549418] target [0.] loss 350.87115478515625\n",
      "13 56 output [0.28570232] target [1.] loss 334.582275390625\n",
      "13 57 output [1.1544168] target [0.] loss 363.6160583496094\n",
      "13 58 output [1.0989171] target [1.] loss 262.35345458984375\n",
      "13 59 output [1.1541231] target [0.] loss 315.5501708984375\n",
      "13 60 output [1.1537049] target [0.] loss 323.503173828125\n",
      "13 61 output [1.1534337] target [1.] loss 341.3213195800781\n",
      "13 62 output [1.15336] target [3.] loss 302.47235107421875\n",
      "13 63 output [0.16096754] target [0.] loss 293.2567138671875\n",
      "13 64 output [1.1418567] target [0.] loss 384.07861328125\n",
      "13 65 output [1.1516482] target [2.] loss 307.7170715332031\n",
      "13 66 output [1.1100445] target [2.] loss 340.5146789550781\n",
      "13 67 output [1.1182959] target [0.] loss 268.7805480957031\n",
      "13 68 output [1.1504364] target [1.] loss 307.289306640625\n",
      "13 69 output [1.1498536] target [2.] loss 375.6124572753906\n",
      "13 70 output [1.1494691] target [4.] loss 311.7351379394531\n",
      "13 71 output [1.1289515] target [0.] loss 291.03790283203125\n",
      "13 72 output [1.149218] target [0.] loss 286.9842224121094\n",
      "Training [70%]\tLoss: 286.9842 Elapsed 4138.163284 total elapsed 31459.406301000003\n",
      "14 0 output [1.1473985] target [2.] loss 388.35931396484375\n",
      "14 1 output [1.1486329] target [0.] loss 333.4666748046875\n",
      "14 2 output [1.2014718] target [0.] loss 329.1662902832031\n",
      "14 3 output [1.1485413] target [2.] loss 300.38653564453125\n",
      "14 4 output [1.1485738] target [0.] loss 329.11517333984375\n",
      "14 5 output [1.1381358] target [1.] loss 350.4158935546875\n",
      "14 6 output [1.1488961] target [1.] loss 327.638916015625\n",
      "14 7 output [0.9988663] target [0.] loss 315.3901062011719\n",
      "14 8 output [1.148963] target [2.] loss 382.49090576171875\n",
      "14 9 output [0.79218566] target [0.] loss 298.31866455078125\n",
      "14 10 output [1.1509017] target [0.] loss 351.37744140625\n",
      "14 11 output [1.151352] target [1.] loss 341.14892578125\n",
      "14 12 output [1.0945891] target [0.] loss 284.71038818359375\n",
      "14 13 output [1.1525061] target [3.] loss 301.9157409667969\n",
      "14 14 output [0.67757124] target [3.] loss 340.9747314453125\n",
      "14 15 output [1.15387] target [0.] loss 244.32754516601562\n",
      "14 16 output [1.1536132] target [0.] loss 247.44052124023438\n",
      "14 17 output [1.1531391] target [2.] loss 295.72039794921875\n",
      "14 18 output [1.1523997] target [1.] loss 279.1943054199219\n",
      "14 19 output [1.1508013] target [1.] loss 306.04864501953125\n",
      "14 20 output [1.102515] target [1.] loss 301.3162536621094\n",
      "14 21 output [1.1492757] target [3.] loss 299.58172607421875\n",
      "14 22 output [1.1177839] target [0.] loss 313.6962890625\n",
      "14 23 output [1.1468576] target [0.] loss 293.1849060058594\n",
      "14 24 output [1.1432374] target [0.] loss 328.27862548828125\n",
      "14 25 output [1.1436741] target [2.] loss 317.4374694824219\n",
      "14 26 output [0.27975604] target [2.] loss 337.8168640136719\n",
      "14 27 output [1.1419683] target [3.] loss 290.0986633300781\n",
      "14 28 output [1.1414218] target [1.] loss 341.85699462890625\n",
      "14 29 output [1.1399314] target [2.] loss 323.922607421875\n",
      "14 30 output [1.0977926] target [0.] loss 350.0716552734375\n",
      "14 31 output [1.1402597] target [2.] loss 365.1707458496094\n",
      "14 32 output [1.1402882] target [1.] loss 314.9056396484375\n",
      "14 33 output [1.1410906] target [0.] loss 329.36834716796875\n",
      "14 34 output [1.5480019] target [1.] loss 421.552734375\n",
      "14 35 output [1.1430554] target [0.] loss 348.85076904296875\n",
      "14 36 output [1.1441774] target [2.] loss 335.73095703125\n",
      "14 37 output [1.1458237] target [2.] loss 333.8721618652344\n",
      "14 38 output [1.1399251] target [2.] loss 357.7237854003906\n",
      "14 39 output [0.6939646] target [0.] loss 371.2700500488281\n",
      "14 40 output [0.4655291] target [0.] loss 322.278076171875\n",
      "14 41 output [1.1518962] target [1.] loss 370.4914245605469\n",
      "14 42 output [1.1564004] target [2.] loss 331.03118896484375\n",
      "14 43 output [0.6601863] target [1.] loss 294.7307434082031\n",
      "14 44 output [1.1610316] target [0.] loss 280.8346252441406\n",
      "14 45 output [1.1634641] target [4.] loss 347.64239501953125\n",
      "14 46 output [1.1658156] target [1.] loss 305.55279541015625\n",
      "14 47 output [1.1055405] target [2.] loss 375.1025390625\n",
      "14 48 output [1.1681671] target [1.] loss 411.08758544921875\n",
      "14 49 output [1.1691957] target [0.] loss 339.7185363769531\n",
      "14 50 output [1.1699892] target [1.] loss 345.21282958984375\n",
      "14 51 output [1.1719747] target [1.] loss 419.2633056640625\n",
      "14 52 output [1.1737999] target [1.] loss 291.10546875\n",
      "14 53 output [1.1745155] target [1.] loss 288.588134765625\n",
      "14 54 output [0.13017465] target [2.] loss 344.1893310546875\n",
      "14 55 output [0.9830312] target [1.] loss 313.69036865234375\n",
      "14 56 output [1.1757468] target [1.] loss 356.71942138671875\n",
      "14 57 output [1.1756136] target [0.] loss 330.646728515625\n",
      "14 58 output [1.1752672] target [0.] loss 319.28924560546875\n",
      "14 59 output [1.7626752] target [1.] loss 305.77301025390625\n",
      "14 60 output [1.1741701] target [0.] loss 421.99786376953125\n",
      "14 61 output [1.173823] target [1.] loss 338.47589111328125\n",
      "14 62 output [1.1737179] target [1.] loss 394.0743103027344\n",
      "14 63 output [1.1400265] target [2.] loss 308.80572509765625\n",
      "14 64 output [1.1747547] target [1.] loss 327.95013427734375\n",
      "14 65 output [1.1284695] target [0.] loss 318.2292175292969\n",
      "14 66 output [1.1754415] target [3.] loss 335.25323486328125\n",
      "14 67 output [1.175789] target [2.] loss 290.6780090332031\n",
      "14 68 output [1.17487] target [0.] loss 370.86981201171875\n",
      "14 69 output [1.1764176] target [2.] loss 306.8742370605469\n",
      "14 70 output [0.23994868] target [4.] loss 303.25714111328125\n",
      "14 71 output [1.1753988] target [2.] loss 358.619384765625\n",
      "14 72 output [1.1694963] target [2.] loss 307.38818359375\n",
      "Training [75%]\tLoss: 307.3882 Elapsed 4427.040369 total elapsed 35886.446670000005\n",
      "15 0 output [0.46957588] target [1.] loss 271.19671630859375\n",
      "15 1 output [1.4915833] target [0.] loss 302.24334716796875\n",
      "15 2 output [1.1760895] target [2.] loss 423.2888488769531\n",
      "15 3 output [1.1757197] target [2.] loss 320.21844482421875\n",
      "15 4 output [0.3104564] target [3.] loss 324.63861083984375\n",
      "15 5 output [1.1745524] target [0.] loss 335.6532897949219\n",
      "15 6 output [1.6177844] target [2.] loss 301.9691162109375\n",
      "15 7 output [1.1707331] target [0.] loss 320.5882568359375\n",
      "15 8 output [1.1714959] target [2.] loss 302.61383056640625\n",
      "15 9 output [1.1697565] target [0.] loss 332.7249450683594\n",
      "15 10 output [1.1589987] target [0.] loss 317.6836853027344\n",
      "15 11 output [1.1654094] target [2.] loss 354.53662109375\n",
      "15 12 output [1.1638129] target [1.] loss 294.263671875\n",
      "15 13 output [1.1618078] target [3.] loss 322.22454833984375\n",
      "15 14 output [1.1179651] target [2.] loss 308.89288330078125\n",
      "15 15 output [1.1578766] target [0.] loss 349.1078796386719\n",
      "15 16 output [1.1561134] target [0.] loss 386.72418212890625\n",
      "15 17 output [1.1551088] target [1.] loss 368.5927734375\n",
      "15 18 output [1.1558231] target [1.] loss 334.6380615234375\n",
      "15 19 output [1.1525629] target [2.] loss 252.58612060546875\n",
      "15 20 output [1.4663135] target [1.] loss 274.5879211425781\n",
      "15 21 output [1.1557004] target [0.] loss 324.684814453125\n",
      "15 22 output [1.1552573] target [2.] loss 346.4647216796875\n",
      "15 23 output [1.1552566] target [0.] loss 394.02532958984375\n",
      "15 24 output [1.1541724] target [1.] loss 304.3603515625\n",
      "15 25 output [1.1541884] target [2.] loss 284.637939453125\n",
      "15 26 output [1.4055393] target [2.] loss 311.18438720703125\n",
      "15 27 output [1.151786] target [2.] loss 387.033203125\n",
      "15 28 output [1.1510434] target [1.] loss 353.1048583984375\n",
      "15 29 output [1.1507409] target [1.] loss 358.92108154296875\n",
      "15 30 output [0.17683522] target [0.] loss 341.3059997558594\n",
      "15 31 output [1.1378922] target [2.] loss 296.098876953125\n",
      "15 32 output [1.141169] target [2.] loss 371.3555908203125\n",
      "15 33 output [1.1552398] target [1.] loss 283.87811279296875\n",
      "15 34 output [1.1567589] target [1.] loss 365.80645751953125\n",
      "15 35 output [1.1577463] target [1.] loss 357.64007568359375\n",
      "15 36 output [1.2170237] target [0.] loss 286.9546813964844\n",
      "15 37 output [1.1610782] target [0.] loss 360.09698486328125\n",
      "15 38 output [1.1625855] target [0.] loss 308.3874206542969\n",
      "15 39 output [1.1638525] target [1.] loss 235.70115661621094\n",
      "15 40 output [0.63676894] target [0.] loss 308.64239501953125\n",
      "15 41 output [1.1564509] target [0.] loss 290.86529541015625\n",
      "15 42 output [1.1653749] target [3.] loss 304.8251647949219\n",
      "15 43 output [1.1656766] target [0.] loss 351.7480773925781\n",
      "15 44 output [1.1651056] target [0.] loss 247.69015502929688\n",
      "15 45 output [1.1655375] target [3.] loss 342.6927490234375\n",
      "15 46 output [1.1657879] target [1.] loss 326.1826171875\n",
      "15 47 output [0.6989475] target [0.] loss 329.8096008300781\n",
      "15 48 output [1.1652787] target [1.] loss 340.1798400878906\n",
      "15 49 output [1.2893277] target [0.] loss 299.6558837890625\n",
      "15 50 output [0.26265678] target [0.] loss 344.0190734863281\n",
      "15 51 output [1.1283283] target [2.] loss 316.72900390625\n",
      "15 52 output [1.1634381] target [0.] loss 359.6055908203125\n",
      "15 53 output [1.1627332] target [0.] loss 319.89093017578125\n",
      "15 54 output [1.1613735] target [0.] loss 349.4638366699219\n",
      "15 55 output [1.1605794] target [1.] loss 337.0860900878906\n",
      "15 56 output [1.1538149] target [1.] loss 346.72869873046875\n",
      "15 57 output [1.1557293] target [0.] loss 315.29864501953125\n",
      "15 58 output [0.76706904] target [3.] loss 277.14666748046875\n",
      "15 59 output [1.155806] target [0.] loss 263.20343017578125\n",
      "15 60 output [1.1575484] target [0.] loss 403.2073974609375\n",
      "15 61 output [1.1533773] target [0.] loss 326.22515869140625\n",
      "15 62 output [1.1530064] target [2.] loss 324.6213684082031\n",
      "15 63 output [1.4649838] target [1.] loss 304.3046875\n",
      "15 64 output [1.5737467] target [2.] loss 328.3083190917969\n",
      "15 65 output [0.7801557] target [1.] loss 348.3229064941406\n",
      "15 66 output [1.151411] target [1.] loss 332.6991271972656\n",
      "15 67 output [0.87704146] target [1.] loss 338.5575866699219\n",
      "15 68 output [1.1517109] target [2.] loss 356.3669738769531\n",
      "15 69 output [1.1242115] target [2.] loss 356.98406982421875\n",
      "15 70 output [1.1529863] target [0.] loss 308.1869201660156\n",
      "15 71 output [1.5039787] target [0.] loss 364.82763671875\n",
      "15 72 output [1.1424351] target [0.] loss 298.2471008300781\n",
      "Training [80%]\tLoss: 298.2471 Elapsed 4713.964192 total elapsed 40600.410862000004\n",
      "16 0 output [1.1557512] target [1.] loss 339.38006591796875\n",
      "16 1 output [1.1270127] target [0.] loss 305.9516296386719\n",
      "16 2 output [1.1552162] target [0.] loss 336.5393981933594\n",
      "16 3 output [1.1292139] target [0.] loss 320.1014404296875\n",
      "16 4 output [1.1548852] target [0.] loss 316.1636657714844\n",
      "16 5 output [0.62515473] target [2.] loss 328.2742614746094\n",
      "16 6 output [1.1270452] target [0.] loss 324.65484619140625\n",
      "16 7 output [1.1395905] target [0.] loss 334.1263122558594\n",
      "16 8 output [1.1542269] target [1.] loss 244.64471435546875\n",
      "16 9 output [1.6517286] target [1.] loss 327.470947265625\n",
      "16 10 output [1.1049917] target [1.] loss 350.444580078125\n",
      "16 11 output [1.1514673] target [2.] loss 300.74102783203125\n",
      "16 12 output [1.1495135] target [2.] loss 307.8229675292969\n",
      "16 13 output [1.1495013] target [2.] loss 325.64483642578125\n",
      "16 14 output [1.1482759] target [2.] loss 277.63299560546875\n",
      "16 15 output [1.1471353] target [1.] loss 300.81793212890625\n",
      "16 16 output [1.1448545] target [0.] loss 364.6973876953125\n",
      "16 17 output [1.1390427] target [1.] loss 318.41802978515625\n",
      "16 18 output [1.2815094] target [2.] loss 292.3360900878906\n",
      "16 19 output [1.142964] target [0.] loss 327.8394470214844\n",
      "16 20 output [1.1417757] target [2.] loss 375.4765625\n",
      "16 21 output [1.2220701] target [3.] loss 296.65277099609375\n",
      "16 22 output [1.1394963] target [3.] loss 316.14990234375\n",
      "16 23 output [1.419712] target [2.] loss 311.49261474609375\n",
      "16 24 output [1.1375923] target [0.] loss 317.1490478515625\n",
      "16 25 output [1.1370786] target [1.] loss 410.23956298828125\n",
      "16 26 output [0.39496696] target [2.] loss 334.05853271484375\n",
      "16 27 output [1.137367] target [1.] loss 276.9208068847656\n",
      "16 28 output [1.1368728] target [1.] loss 309.2490234375\n",
      "16 29 output [1.1370661] target [1.] loss 360.3523864746094\n",
      "16 30 output [1.1376354] target [1.] loss 320.3828125\n",
      "16 31 output [0.89146876] target [1.] loss 364.28192138671875\n",
      "16 32 output [1.1392394] target [1.] loss 309.1943664550781\n",
      "16 33 output [1.1395848] target [0.] loss 304.5485534667969\n",
      "16 34 output [0.8246375] target [3.] loss 345.5628356933594\n",
      "16 35 output [1.1390282] target [2.] loss 401.0570068359375\n",
      "16 36 output [0.56885934] target [1.] loss 411.3602600097656\n",
      "16 37 output [0.79799235] target [2.] loss 303.96490478515625\n",
      "16 38 output [1.1394986] target [2.] loss 337.2838134765625\n",
      "16 39 output [0.44944817] target [1.] loss 397.3103942871094\n",
      "16 40 output [1.1690754] target [1.] loss 280.7716064453125\n",
      "16 41 output [1.1399425] target [0.] loss 306.4297180175781\n",
      "16 42 output [1.2647963] target [2.] loss 388.392333984375\n",
      "16 43 output [1.1215534] target [0.] loss 264.48077392578125\n",
      "16 44 output [1.1411202] target [3.] loss 300.67962646484375\n",
      "16 45 output [1.1408349] target [1.] loss 313.8046875\n",
      "16 46 output [1.1403346] target [1.] loss 329.2577209472656\n",
      "16 47 output [1.1399119] target [0.] loss 325.36651611328125\n",
      "16 48 output [1.1394012] target [2.] loss 373.5088195800781\n",
      "16 49 output [1.1365025] target [0.] loss 305.2500915527344\n",
      "16 50 output [1.1391394] target [2.] loss 297.9668884277344\n",
      "16 51 output [1.1391867] target [1.] loss 280.7681884765625\n",
      "16 52 output [1.1381559] target [3.] loss 375.36541748046875\n",
      "16 53 output [1.1377255] target [2.] loss 306.7595520019531\n",
      "16 54 output [1.1366922] target [2.] loss 296.4659423828125\n",
      "16 55 output [0.64044654] target [3.] loss 333.62384033203125\n",
      "16 56 output [1.1357741] target [1.] loss 295.66717529296875\n",
      "16 57 output [1.1345614] target [1.] loss 305.88232421875\n",
      "16 58 output [1.1340739] target [0.] loss 296.09814453125\n",
      "16 59 output [1.1329165] target [2.] loss 292.10589599609375\n",
      "16 60 output [1.1315122] target [1.] loss 375.31390380859375\n",
      "16 61 output [1.1305209] target [0.] loss 322.13494873046875\n",
      "16 62 output [1.122066] target [0.] loss 284.1051940917969\n",
      "16 63 output [1.1177056] target [0.] loss 292.28729248046875\n",
      "16 64 output [1.1273348] target [1.] loss 284.69964599609375\n",
      "16 65 output [1.1264766] target [2.] loss 292.5966796875\n",
      "16 66 output [1.1259816] target [1.] loss 399.8323974609375\n",
      "16 67 output [1.1259594] target [0.] loss 351.43646240234375\n",
      "16 68 output [1.1259205] target [1.] loss 320.0826416015625\n",
      "16 69 output [1.1256112] target [2.] loss 366.8961486816406\n",
      "16 70 output [1.0968387] target [1.] loss 319.3548583984375\n",
      "16 71 output [1.1271298] target [0.] loss 343.54815673828125\n",
      "16 72 output [1.123356] target [2.] loss 348.1993103027344\n",
      "Training [85%]\tLoss: 348.1993 Elapsed 4995.893379 total elapsed 45596.304241000005\n",
      "17 0 output [1.1287601] target [1.] loss 332.3753662109375\n",
      "17 1 output [1.1296757] target [0.] loss 316.80047607421875\n",
      "17 2 output [1.1212466] target [0.] loss 273.2647705078125\n",
      "17 3 output [1.1311091] target [0.] loss 342.68829345703125\n",
      "17 4 output [1.1316719] target [1.] loss 296.48321533203125\n",
      "17 5 output [1.1321466] target [0.] loss 336.23126220703125\n",
      "17 6 output [1.1322861] target [2.] loss 311.6582946777344\n",
      "17 7 output [1.1326934] target [1.] loss 358.59051513671875\n",
      "17 8 output [1.133501] target [0.] loss 319.089599609375\n",
      "17 9 output [1.1340802] target [1.] loss 304.38531494140625\n",
      "17 10 output [1.1347497] target [0.] loss 384.4305419921875\n",
      "17 11 output [1.1352512] target [1.] loss 258.8301086425781\n",
      "17 12 output [1.1854289] target [3.] loss 313.5538024902344\n",
      "17 13 output [1.1365534] target [2.] loss 312.1131591796875\n",
      "17 14 output [1.1363215] target [1.] loss 312.2073974609375\n",
      "17 15 output [1.139025] target [0.] loss 317.1897277832031\n",
      "17 16 output [1.1400571] target [3.] loss 304.96142578125\n",
      "17 17 output [1.1413984] target [1.] loss 326.1415100097656\n",
      "17 18 output [0.67489713] target [0.] loss 338.8764343261719\n",
      "17 19 output [1.1280522] target [0.] loss 319.0182800292969\n",
      "17 20 output [1.4472108] target [3.] loss 339.1009216308594\n",
      "17 21 output [1.1441278] target [2.] loss 273.0977783203125\n",
      "17 22 output [1.1459115] target [1.] loss 292.33428955078125\n",
      "17 23 output [1.1592546] target [1.] loss 319.5852966308594\n",
      "17 24 output [1.1465057] target [2.] loss 349.5145568847656\n",
      "17 25 output [1.1423877] target [0.] loss 363.34515380859375\n",
      "17 26 output [1.1312336] target [1.] loss 316.3636474609375\n",
      "17 27 output [1.1470811] target [0.] loss 308.92388916015625\n",
      "17 28 output [1.1292269] target [0.] loss 304.3258056640625\n",
      "17 29 output [1.1466119] target [1.] loss 272.14276123046875\n",
      "17 30 output [1.1460505] target [2.] loss 310.0558166503906\n",
      "17 31 output [1.1456875] target [0.] loss 303.616455078125\n",
      "17 32 output [1.145203] target [0.] loss 339.9007568359375\n",
      "17 33 output [1.1446271] target [1.] loss 344.44091796875\n",
      "17 34 output [1.1441524] target [2.] loss 301.7007141113281\n",
      "17 35 output [1.1439201] target [2.] loss 349.17608642578125\n",
      "17 36 output [1.4563527] target [0.] loss 367.06884765625\n",
      "17 37 output [1.1430606] target [0.] loss 382.72723388671875\n",
      "17 38 output [1.1428347] target [0.] loss 294.42523193359375\n",
      "17 39 output [1.1212859] target [0.] loss 332.8169250488281\n",
      "17 40 output [1.1410056] target [0.] loss 352.205078125\n",
      "17 41 output [1.1401927] target [1.] loss 350.1622619628906\n",
      "17 42 output [1.1387033] target [1.] loss 334.488525390625\n",
      "17 43 output [1.1391697] target [2.] loss 415.6756896972656\n",
      "17 44 output [1.1300563] target [1.] loss 332.0368347167969\n",
      "17 45 output [1.1405532] target [1.] loss 301.67413330078125\n",
      "17 46 output [1.1410636] target [1.] loss 357.0564880371094\n",
      "17 47 output [0.7017737] target [0.] loss 297.8863830566406\n",
      "17 48 output [1.1416935] target [1.] loss 395.6204528808594\n",
      "17 49 output [0.20392796] target [1.] loss 384.2546081542969\n",
      "17 50 output [1.1431494] target [0.] loss 322.09722900390625\n",
      "17 51 output [1.4204211] target [3.] loss 301.4462890625\n",
      "17 52 output [1.1439729] target [4.] loss 322.13507080078125\n",
      "17 53 output [1.1440225] target [0.] loss 386.68634033203125\n",
      "17 54 output [1.144273] target [1.] loss 322.4319763183594\n",
      "17 55 output [1.1444238] target [0.] loss 328.760498046875\n",
      "17 56 output [1.1414235] target [0.] loss 291.5367431640625\n",
      "17 57 output [1.1445315] target [0.] loss 312.267578125\n",
      "17 58 output [1.144559] target [4.] loss 265.630859375\n",
      "17 59 output [0.43718094] target [1.] loss 296.86517333984375\n",
      "17 60 output [1.136537] target [1.] loss 328.47491455078125\n",
      "17 61 output [0.07722765] target [4.] loss 317.736083984375\n",
      "17 62 output [1.1444596] target [0.] loss 312.455078125\n",
      "17 63 output [1.1444938] target [1.] loss 323.79534912109375\n",
      "17 64 output [1.1440865] target [4.] loss 345.425048828125\n",
      "17 65 output [1.1044309] target [2.] loss 292.3170471191406\n",
      "17 66 output [1.144479] target [0.] loss 380.76300048828125\n",
      "17 67 output [1.1445885] target [0.] loss 337.0973815917969\n",
      "17 68 output [1.1449792] target [1.] loss 362.68438720703125\n",
      "17 69 output [1.1452492] target [4.] loss 307.9370422363281\n",
      "17 70 output [0.64441484] target [1.] loss 336.25634765625\n",
      "17 71 output [1.2788262] target [1.] loss 260.36328125\n",
      "17 72 output [1.5430417] target [0.] loss 323.769775390625\n",
      "Training [90%]\tLoss: 323.7698 Elapsed 5272.207208 total elapsed 50868.511449000005\n",
      "18 0 output [1.1459203] target [1.] loss 306.61773681640625\n",
      "18 1 output [1.1464428] target [0.] loss 361.79248046875\n",
      "18 2 output [1.1475515] target [2.] loss 389.14385986328125\n",
      "18 3 output [1.1491616] target [1.] loss 416.2747802734375\n",
      "18 4 output [1.1504655] target [0.] loss 276.6419982910156\n",
      "18 5 output [1.1514695] target [3.] loss 312.39990234375\n",
      "18 6 output [1.1398532] target [1.] loss 243.84190368652344\n",
      "18 7 output [0.1009777] target [2.] loss 400.938720703125\n",
      "18 8 output [1.1532847] target [0.] loss 304.62933349609375\n",
      "18 9 output [1.1533678] target [0.] loss 297.3109436035156\n",
      "18 10 output [1.1538079] target [3.] loss 364.1419982910156\n",
      "18 11 output [1.1545343] target [1.] loss 283.68505859375\n",
      "18 12 output [1.1545208] target [2.] loss 226.71261596679688\n",
      "18 13 output [0.58197266] target [0.] loss 306.67333984375\n",
      "18 14 output [1.1532285] target [1.] loss 336.6458740234375\n",
      "18 15 output [1.385295] target [2.] loss 317.00140380859375\n",
      "18 16 output [1.1509501] target [2.] loss 283.3125915527344\n",
      "18 17 output [1.1494572] target [1.] loss 378.22186279296875\n",
      "18 18 output [0.2514028] target [4.] loss 330.175537109375\n",
      "18 19 output [1.147543] target [1.] loss 395.1077575683594\n",
      "18 20 output [1.1464133] target [0.] loss 370.9092712402344\n",
      "18 21 output [1.1470593] target [1.] loss 259.3181457519531\n",
      "18 22 output [1.3234894] target [0.] loss 332.1132507324219\n",
      "18 23 output [1.187314] target [0.] loss 293.43182373046875\n",
      "18 24 output [1.1457157] target [2.] loss 266.5622253417969\n",
      "18 25 output [1.1424996] target [2.] loss 367.11663818359375\n",
      "18 26 output [1.12669] target [2.] loss 376.80865478515625\n",
      "18 27 output [1.143005] target [2.] loss 387.49468994140625\n",
      "18 28 output [1.1390382] target [2.] loss 287.5147705078125\n",
      "18 29 output [1.4560847] target [1.] loss 335.3698425292969\n",
      "18 30 output [1.1417208] target [1.] loss 258.66845703125\n",
      "18 31 output [0.29885617] target [0.] loss 346.75732421875\n",
      "18 32 output [1.1402483] target [1.] loss 344.2814025878906\n",
      "18 33 output [1.1398505] target [1.] loss 341.6186828613281\n",
      "18 34 output [1.1385508] target [0.] loss 311.4601135253906\n",
      "18 35 output [0.52768326] target [0.] loss 451.93829345703125\n",
      "18 36 output [1.1387832] target [0.] loss 351.28826904296875\n",
      "18 37 output [1.361902] target [2.] loss 299.876953125\n",
      "18 38 output [1.3545667] target [0.] loss 339.3206481933594\n",
      "18 39 output [1.1398232] target [2.] loss 372.94927978515625\n",
      "18 40 output [1.140448] target [1.] loss 303.6861267089844\n",
      "18 41 output [1.141036] target [2.] loss 361.1942138671875\n",
      "18 42 output [1.1393127] target [1.] loss 335.00555419921875\n",
      "18 43 output [0.9791802] target [1.] loss 381.651123046875\n",
      "18 44 output [1.1422297] target [0.] loss 350.705322265625\n",
      "18 45 output [1.1431079] target [0.] loss 350.5899963378906\n",
      "18 46 output [1.1432474] target [1.] loss 374.6571350097656\n",
      "18 47 output [1.143576] target [1.] loss 423.8271484375\n",
      "18 48 output [1.1442636] target [0.] loss 325.04443359375\n",
      "18 49 output [1.1449363] target [4.] loss 361.60614013671875\n",
      "18 50 output [1.1454966] target [2.] loss 316.2166748046875\n",
      "18 51 output [1.146287] target [0.] loss 335.9254455566406\n",
      "18 52 output [1.1469407] target [1.] loss 281.51837158203125\n",
      "18 53 output [1.1468288] target [0.] loss 292.17950439453125\n",
      "18 54 output [0.5902337] target [2.] loss 409.1413879394531\n",
      "18 55 output [1.1470261] target [3.] loss 324.79962158203125\n",
      "18 56 output [1.1470454] target [2.] loss 344.113525390625\n",
      "18 57 output [1.6207318] target [1.] loss 392.5274353027344\n",
      "18 58 output [1.1483291] target [1.] loss 365.3150329589844\n",
      "18 59 output [1.1493429] target [2.] loss 349.18505859375\n",
      "18 60 output [1.1508089] target [0.] loss 325.4530944824219\n",
      "18 61 output [1.1520855] target [0.] loss 316.6724853515625\n",
      "18 62 output [1.1525358] target [2.] loss 397.25787353515625\n",
      "18 63 output [0.68401384] target [0.] loss 343.7905578613281\n",
      "18 64 output [1.1535255] target [0.] loss 331.0874328613281\n",
      "18 65 output [1.1544048] target [0.] loss 322.32550048828125\n",
      "18 66 output [0.43203378] target [1.] loss 364.5081481933594\n",
      "18 67 output [1.1545314] target [1.] loss 327.63226318359375\n",
      "18 68 output [1.1541187] target [2.] loss 289.396484375\n",
      "18 69 output [1.1535187] target [3.] loss 259.92584228515625\n",
      "18 70 output [1.1530728] target [1.] loss 346.53759765625\n",
      "18 71 output [1.1525527] target [2.] loss 306.6566162109375\n",
      "18 72 output [1.2818501] target [1.] loss 276.40155029296875\n",
      "Training [95%]\tLoss: 276.4016 Elapsed 5562.023052 total elapsed 56430.534501\n",
      "19 0 output [1.1527615] target [0.] loss 315.6533203125\n",
      "19 1 output [1.1487162] target [1.] loss 359.2147521972656\n",
      "19 2 output [1.1468194] target [0.] loss 321.801025390625\n",
      "19 3 output [1.1447712] target [0.] loss 384.957763671875\n",
      "19 4 output [1.142925] target [0.] loss 315.41290283203125\n",
      "19 5 output [1.1398726] target [0.] loss 307.8480529785156\n",
      "19 6 output [1.1401691] target [1.] loss 293.26068115234375\n",
      "19 7 output [1.1391952] target [1.] loss 324.12738037109375\n",
      "19 8 output [1.1373885] target [1.] loss 299.9327392578125\n",
      "19 9 output [1.1358993] target [1.] loss 337.926025390625\n",
      "19 10 output [1.1348267] target [0.] loss 301.56634521484375\n",
      "19 11 output [1.1367909] target [2.] loss 315.9920959472656\n",
      "19 12 output [1.1332679] target [1.] loss 324.3255615234375\n",
      "19 13 output [1.132874] target [3.] loss 276.8069152832031\n",
      "19 14 output [1.1319584] target [0.] loss 321.7130126953125\n",
      "19 15 output [1.131517] target [0.] loss 301.9758605957031\n",
      "19 16 output [1.1309124] target [1.] loss 322.3871765136719\n",
      "19 17 output [1.2902839] target [4.] loss 344.3638916015625\n",
      "19 18 output [1.129915] target [2.] loss 343.4810485839844\n",
      "19 19 output [1.1295313] target [2.] loss 296.37188720703125\n",
      "19 20 output [1.1292224] target [1.] loss 299.85809326171875\n",
      "19 21 output [1.1301107] target [4.] loss 339.7796630859375\n",
      "19 22 output [1.1286173] target [0.] loss 320.14349365234375\n",
      "19 23 output [1.1281792] target [0.] loss 354.8354797363281\n",
      "19 24 output [1.1283001] target [1.] loss 336.1649169921875\n",
      "19 25 output [1.1291705] target [1.] loss 302.04742431640625\n",
      "19 26 output [1.1289368] target [1.] loss 331.9124450683594\n",
      "19 27 output [1.1292366] target [1.] loss 295.2690734863281\n",
      "19 28 output [1.3677387] target [0.] loss 317.0406494140625\n",
      "19 29 output [1.1290634] target [1.] loss 342.4852294921875\n",
      "19 30 output [1.1290867] target [0.] loss 261.4090576171875\n",
      "19 31 output [0.6624331] target [0.] loss 312.3609619140625\n",
      "19 32 output [1.1274016] target [2.] loss 323.689453125\n",
      "19 33 output [0.7851703] target [0.] loss 374.5162048339844\n",
      "19 34 output [1.1203531] target [0.] loss 360.43603515625\n",
      "19 35 output [1.1266445] target [0.] loss 317.14447021484375\n",
      "19 36 output [1.126843] target [1.] loss 276.1378173828125\n",
      "19 37 output [1.1275895] target [0.] loss 327.2999267578125\n",
      "19 38 output [1.1273525] target [1.] loss 332.29248046875\n",
      "19 39 output [1.1278528] target [1.] loss 431.27197265625\n",
      "19 40 output [1.1287247] target [2.] loss 327.68536376953125\n",
      "19 41 output [1.1296175] target [2.] loss 339.89508056640625\n",
      "19 42 output [1.1310601] target [1.] loss 285.72991943359375\n",
      "19 43 output [1.0115373] target [1.] loss 353.18634033203125\n",
      "19 44 output [1.1354299] target [2.] loss 312.17816162109375\n",
      "19 45 output [1.4158152] target [2.] loss 308.2056579589844\n",
      "19 46 output [1.1391203] target [1.] loss 357.58953857421875\n",
      "19 47 output [1.1408148] target [0.] loss 263.80694580078125\n",
      "19 48 output [0.9543584] target [0.] loss 334.30596923828125\n",
      "19 49 output [1.142736] target [1.] loss 315.08673095703125\n",
      "19 50 output [1.143195] target [0.] loss 303.12237548828125\n",
      "19 51 output [1.1435453] target [1.] loss 318.7718505859375\n",
      "19 52 output [1.1439321] target [2.] loss 288.4649658203125\n",
      "19 53 output [1.1440101] target [0.] loss 381.5657958984375\n",
      "19 54 output [0.34491694] target [1.] loss 266.2868347167969\n",
      "19 55 output [1.1421843] target [2.] loss 297.41546630859375\n",
      "19 56 output [1.1432954] target [1.] loss 322.7845458984375\n",
      "19 57 output [1.1419806] target [2.] loss 296.1983947753906\n",
      "19 58 output [1.140744] target [0.] loss 255.22634887695312\n",
      "19 59 output [1.1397839] target [1.] loss 313.2890625\n",
      "19 60 output [1.0608575] target [5.] loss 344.13385009765625\n",
      "19 61 output [1.138397] target [1.] loss 362.0262145996094\n",
      "19 62 output [1.1373351] target [1.] loss 347.33673095703125\n",
      "19 63 output [1.1366339] target [0.] loss 310.4452209472656\n",
      "19 64 output [1.1381474] target [4.] loss 300.35504150390625\n",
      "19 65 output [1.1349765] target [1.] loss 298.3401794433594\n",
      "19 66 output [1.1342957] target [1.] loss 330.3822021484375\n",
      "19 67 output [1.1339161] target [2.] loss 314.46282958984375\n",
      "19 68 output [1.1335547] target [0.] loss 289.5919189453125\n",
      "19 69 output [1.1333466] target [0.] loss 304.62799072265625\n",
      "19 70 output [1.132611] target [1.] loss 354.2396240234375\n",
      "19 71 output [0.83186746] target [1.] loss 305.4869384765625\n",
      "19 72 output [1.1316311] target [0.] loss 349.05206298828125\n",
      "Training [100%]\tLoss: 349.0521 Elapsed 5863.699665 total elapsed 62294.234166\n"
     ]
    }
   ],
   "source": [
    "# Define model, optimizer, and loss function\n",
    "start_time = datetime.now()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = LOSS_FUNC\n",
    "\n",
    "# Start training\n",
    "epochs = EPOCHS  # Set number of epochs\n",
    "model.train()  # Set model to training mode\n",
    "# m = Softmax(dim=3)\n",
    "\n",
    "print(DATASET, QUBITS, \"Qubits, \", training_stats[0], \"prior epochs, \", \"Accuracy\", '{0:.1f}%'.format(training_stats[1]), \"Epochs\", EPOCHS, \"Batch Size:\", BATCH_SIZE)\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx in range(73):\n",
    "      imgs_batch, res_batch, real_batch = dataset.minibatch(BATCH_SIZE)\n",
    "      # for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      if(resume_stats[0] > epoch or (resume_stats[0] == epoch and resume_stats[1] >= batch_idx)):\n",
    "        print(epoch, batch_idx, \"already processed\")\n",
    "        continue\n",
    "      optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "      if(DEBUG):\n",
    "        print(\"epoch\", epoch, \"batch\", batch_idx)\n",
    "      # print(\"indexes\", batch_idx*BATCH_SIZE, batch_idx*BATCH_SIZE+BATCH_SIZE)\n",
    "      # features = train_features[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE, :, :, :]\n",
    "      # print(\"features\", features[0, :, 0, 0])\n",
    "      # print(\"x\", x[0, :, 0, 0])\n",
    "      data = torch.from_numpy(imgs_batch).to(device)\n",
    "      target = torch.from_numpy(res_batch[:, NEURONS_FROM:NEURONS_FROM+NEURONS_PREDICTED]).to(device)\n",
    "      # print(\"target\", target.shape)\n",
    "\n",
    "      def closure():\n",
    "          optimizer.zero_grad()\n",
    "          output = model(data)\n",
    "          loss = loss_func(output, target)\n",
    "          print(epoch, batch_idx, \"output\", output[0].cpu().detach().numpy(), \"target\", target[0].cpu().detach().numpy(), \"loss\", loss.item())\n",
    "          loss_list.append([epoch, batch_idx, loss.item()])\n",
    "          loss.backward()\n",
    "          return loss\n",
    "\n",
    "      optimizer.step(closure)  # Optimize weights\n",
    "      resume_stats[0] = epoch\n",
    "      resume_stats[1] = batch_idx\n",
    "      torch.save(loss_list, f\"checkpoints/{FILENAME_PREFIX}-losslist.pt\")\n",
    "      torch.save(model.state_dict(), f'checkpoints/{FILENAME_PREFIX}-modelweights.pt')\n",
    "      torch.save(resume_stats, f\"checkpoints/{FILENAME_PREFIX}-trainresumestats.pt\")\n",
    "      if(SAMPLE_RUN == True and batch_idx >= SAMPLE_ITERATIONS):\n",
    "        break\n",
    "    end_time = datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    training_stats[0] = training_stats[0] + 1\n",
    "    if(len(training_stats)<3):\n",
    "      training_stats.append(0)\n",
    "    training_stats[2] = training_stats[2] + elapsed.total_seconds()\n",
    "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1][2]), \"Elapsed\", elapsed.total_seconds(), \"total elapsed\", training_stats[2])\n",
    "\n",
    "    torch.save(model.state_dict(), f'checkpoints/{FILENAME_PREFIX}-modelweights-{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "spoken-stationery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 15 - Plot loss convergence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHybrid NN Training Convergence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Step 15 - Plot loss convergence\n",
    "plt.plot(loss_list[:, 2])\n",
    "plt.title(\"Hybrid NN Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16 - Load model for evaluation\n",
    "\n",
    "qnn = create_qnn()\n",
    "if DATASET == \"blahblahblah\":\n",
    "  model = CifarNet(qnn4)\n",
    "else:\n",
    "  model = Net(qnn4)\n",
    "model.load_state_dict(torch.load(f'checkpoints/{FILENAME_PREFIX}-modelweights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18 - Predict\n",
    "\n",
    "model.eval()  # set model to evaluation mode\n",
    "with no_grad():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  print(\"Testing Set size:\", len(test_loader.dataset), \"Batch Size:\", BATCH_SIZE, \"Number of batches:\", (len(test_loader.dataset)//BATCH_SIZE)+1)\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # if(batch_idx == 12):\n",
    "    #   break\n",
    "    features = test_features[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE, :, :, :]\n",
    "    if(features.shape[0] == 0):\n",
    "      break\n",
    "    output = model(features)\n",
    "    # print(\"prediction output\", output)\n",
    "    print(total, end=\" \")\n",
    "    if len(output.shape) == 1:\n",
    "        output = output.reshape(1, *output.shape)\n",
    "\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    total += pred.shape[0]\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss = loss_func(output, target)\n",
    "    total_loss.append(loss.item())\n",
    "\n",
    "  print(\n",
    "      \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tDataset: {}, Classes: {}, Features: {}, Qubits: {}, Epochs: {} New Epochs: {}\\n\\tAccuracy: {:.1f}% to {:.1f}%\".format(\n",
    "          sum(total_loss) / len(total_loss), DATASET,\n",
    "          CLASSES, TOTAL_FEATURES, QUBITS, training_stats[0], EPOCHS,\n",
    "          training_stats[1], correct * 100 / total\n",
    "      )\n",
    "  )\n",
    "  training_stats[1] = correct * 100 / total\n",
    "\n",
    "  torch.save(training_stats, f\"checkpoints/{FILENAME_PREFIX}-trainingstats.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit.tools.jupyter\n",
    "\n",
    "%qiskit_version_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
